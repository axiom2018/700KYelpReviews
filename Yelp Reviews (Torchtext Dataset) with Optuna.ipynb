{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7754161,"sourceType":"datasetVersion","datasetId":4533974}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport torch\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-14T13:56:12.441315Z","iopub.execute_input":"2024-04-14T13:56:12.441664Z","iopub.status.idle":"2024-04-14T13:56:16.650970Z","shell.execute_reply.started":"2024-04-14T13:56:12.441635Z","shell.execute_reply":"2024-04-14T13:56:16.649983Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/torchtext-yelpreviewfull/yelp_test.csv\n/kaggle/input/torchtext-yelpreviewfull/yelp_train.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-04-14T13:56:16.653250Z","iopub.execute_input":"2024-04-14T13:56:16.654143Z","iopub.status.idle":"2024-04-14T13:56:16.711023Z","shell.execute_reply.started":"2024-04-14T13:56:16.654102Z","shell.execute_reply":"2024-04-14T13:56:16.709811Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"markdown","source":"# 1) Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/torchtext-yelpreviewfull/yelp_train.csv\", names=['label', 'text'])\ntest_df = pd.read_csv(\"/kaggle/input/torchtext-yelpreviewfull/yelp_test.csv\", names=['label', 'text'])","metadata":{"execution":{"iopub.status.busy":"2024-04-14T13:56:16.712312Z","iopub.execute_input":"2024-04-14T13:56:16.712658Z","iopub.status.idle":"2024-04-14T13:56:28.937989Z","shell.execute_reply.started":"2024-04-14T13:56:16.712626Z","shell.execute_reply":"2024-04-14T13:56:28.937067Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_df.shape, test_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-14T13:56:28.939082Z","iopub.execute_input":"2024-04-14T13:56:28.939366Z","iopub.status.idle":"2024-04-14T13:56:28.945867Z","shell.execute_reply.started":"2024-04-14T13:56:28.939343Z","shell.execute_reply":"2024-04-14T13:56:28.944927Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"((650000, 2), (50000, 2))"},"metadata":{}}]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-14T13:56:28.948902Z","iopub.execute_input":"2024-04-14T13:56:28.949207Z","iopub.status.idle":"2024-04-14T13:56:28.963869Z","shell.execute_reply.started":"2024-04-14T13:56:28.949183Z","shell.execute_reply":"2024-04-14T13:56:28.962965Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   label                                               text\n0      5  dr. goldberg offers everything i look for in a...\n1      2  Unfortunately, the frustration of being Dr. Go...\n2      4  Been going to Dr. Goldberg for over 10 years. ...\n3      4  Got a letter in the mail last week that said D...\n4      1  I don't know what Dr. Goldberg was like before...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>dr. goldberg offers everything i look for in a...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Unfortunately, the frustration of being Dr. Go...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Got a letter in the mail last week that said D...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>I don't know what Dr. Goldberg was like before...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-14T13:56:28.964891Z","iopub.execute_input":"2024-04-14T13:56:28.965165Z","iopub.status.idle":"2024-04-14T13:56:28.974260Z","shell.execute_reply.started":"2024-04-14T13:56:28.965142Z","shell.execute_reply":"2024-04-14T13:56:28.973345Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   label                                               text\n0      1  I got 'new' tires from them and within two wee...\n1      1  Don't waste your time.  We had two different p...\n2      1  All I can say is the worst! We were the only 2...\n3      1  I have been to this restaurant twice and was d...\n4      1  Food was NOT GOOD at all! My husband & I ate h...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>I got 'new' tires from them and within two wee...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Don't waste your time.  We had two different p...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>All I can say is the worst! We were the only 2...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>I have been to this restaurant twice and was d...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>Food was NOT GOOD at all! My husband &amp; I ate h...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Even amount of every label. Definitely a multiclassification dataset.\ntrain_df['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-14T13:56:28.975199Z","iopub.execute_input":"2024-04-14T13:56:28.975457Z","iopub.status.idle":"2024-04-14T13:56:29.003014Z","shell.execute_reply.started":"2024-04-14T13:56:28.975435Z","shell.execute_reply":"2024-04-14T13:56:29.002073Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"label\n5    130000\n2    130000\n4    130000\n1    130000\n3    130000\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"# 2) Preprocessing","metadata":{}},{"cell_type":"code","source":"import torchtext\nfrom torchtext.data.utils import get_tokenizer\n\n\n''' First get tokenizer ready for other function soon. Tokenizers are used in nlp to turn strings/sentences \n    into tokens. Ex: \"The cat is sleeping\" will become ['The', 'cat', 'is', 'sleeping'].\n\n    tk = tokenizer '''\n\ntk = get_tokenizer('basic_english')","metadata":{"execution":{"iopub.status.busy":"2024-04-14T13:56:29.004312Z","iopub.execute_input":"2024-04-14T13:56:29.004960Z","iopub.status.idle":"2024-04-14T13:56:30.556018Z","shell.execute_reply.started":"2024-04-14T13:56:29.004929Z","shell.execute_reply":"2024-04-14T13:56:30.555223Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Example sentence for display\nex_sent = \"This is a cool sentence.\"\n\ntk(ex_sent)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T13:56:30.557172Z","iopub.execute_input":"2024-04-14T13:56:30.557661Z","iopub.status.idle":"2024-04-14T13:56:30.564207Z","shell.execute_reply.started":"2024-04-14T13:56:30.557632Z","shell.execute_reply":"2024-04-14T13:56:30.563094Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"['this', 'is', 'a', 'cool', 'sentence', '.']"},"metadata":{}}]},{"cell_type":"code","source":"''' Steps for preprocessing :\n    \n    1) Lower text - \"What\" and \"what\" will be looked at as 2 different entities if not lowered.\n    \n    2) Remove non number/alpha chars - No need for !, #, etc.\n    \n    3) Tokenization - Explained above\n    \n    4) .join - ' '.join([\"Some\", \"string\", \"here\"]) will produce \"'Some string here'\" so join puts\n        it back together.\n    \n    sent = sentence\n'''\n\ndef preprocessing(sent, print_steps=False):\n    if print_steps is True:\n        print(f'1) --- Current text:\\n{sent}')\n    sent = sent.lower()\n    if print_steps is True:\n        print(f'2) --- Post lower:\\n{sent}')\n    \n    \n    ''' Explanation of regex.\n        \\s - Finds single whitespace char\n        \\w - Matches char in str\n    \n        Observations:\n            1) Doing \" r'^\\w \" - Goes through the entire str, remember the rule google said:\n                \"The search proceeds through the string from start to end\"\n\n            2) re.sub(r'\\w', '', test_sent) - Matches every char, so [a-zA-Z0-9_]. Replaces\n                with nothing. Hence \"'$  @ !!!'\" returned.\n            3) re.sub(r'\\s', '', test_sent) - Matches every white space. And replace it with\n                nothing, hence \"'Th$sis@32434test!!!'\"\n            4) re.sub(r'[\\w\\s]', '', test_sent) - w gets rid of all chars its supposed to like\n                [a-zA-Z0-9_] and also \\s gets rid of all white spaces. Hence \"'$@!!!'\"\n\n            Why does ^ reverse it and remove special chars? \n            Answer: \n            \"An up-hat (^) at the start of a square-bracket set inverts it, so [^ab] means \n            any char except 'a' or 'b'.\" from google source:\n            https://developers.google.com/edu/python/regular-expressions\n            So both \\w\\s say together \"Let's replace all chars [a-zA-Z0-9_] and white spaces\n            with nothing\". BUT, the ^ stops and says do the opposite of that and instead\n            keeps all the matching chars and white spaces. It's genius. '''\n    sent = re.sub(r'[^\\w\\s]', '', sent)\n    if print_steps is True:\n        print(f'3) --- Post regex sub (remove non alpha numerical chars):\\n{sent}')\n    \n    sent = tk(sent)\n    if print_steps is True:\n        print(f'4) --- Post tokenization:\\n{sent}')\n        \n    sent = ' '.join(sent)\n    if print_steps is True:\n        print(f'5) --- Post join:\\n{sent}')\n        \n    return sent\n    \n    \n# Example\npreprocessing(\"Shocking!!! How could group 46 do that?!\", True)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T13:56:30.565668Z","iopub.execute_input":"2024-04-14T13:56:30.565956Z","iopub.status.idle":"2024-04-14T13:56:30.578144Z","shell.execute_reply.started":"2024-04-14T13:56:30.565934Z","shell.execute_reply":"2024-04-14T13:56:30.577195Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"1) --- Current text:\nShocking!!! How could group 46 do that?!\n2) --- Post lower:\nshocking!!! how could group 46 do that?!\n3) --- Post regex sub (remove non alpha numerical chars):\nshocking how could group 46 do that\n4) --- Post tokenization:\n['shocking', 'how', 'could', 'group', '46', 'do', 'that']\n5) --- Post join:\nshocking how could group 46 do that\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'shocking how could group 46 do that'"},"metadata":{}}]},{"cell_type":"code","source":"# Examples from dataframe. Zip can take a range and loop only that many times. \nfor i, x in zip(range(2), train_df['text']):\n    preprocessing(x, True)\n    print('\\n\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2024-04-14T13:56:30.579501Z","iopub.execute_input":"2024-04-14T13:56:30.579821Z","iopub.status.idle":"2024-04-14T13:56:30.590673Z","shell.execute_reply.started":"2024-04-14T13:56:30.579797Z","shell.execute_reply":"2024-04-14T13:56:30.589787Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"1) --- Current text:\ndr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\n2) --- Post lower:\ndr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\n3) --- Post regex sub (remove non alpha numerical chars):\ndr goldberg offers everything i look for in a general practitioner  hes nice and easy to talk to without being patronizing hes always on time in seeing his patients hes affiliated with a topnotch hospital nyu which my parents have explained to me is very important in case something happens and you need surgery and you can get referrals to see specialists without having to see him first  really what more do you need  im sitting here trying to think of any complaints i have about him but im really drawing a blank\n4) --- Post tokenization:\n['dr', 'goldberg', 'offers', 'everything', 'i', 'look', 'for', 'in', 'a', 'general', 'practitioner', 'hes', 'nice', 'and', 'easy', 'to', 'talk', 'to', 'without', 'being', 'patronizing', 'hes', 'always', 'on', 'time', 'in', 'seeing', 'his', 'patients', 'hes', 'affiliated', 'with', 'a', 'topnotch', 'hospital', 'nyu', 'which', 'my', 'parents', 'have', 'explained', 'to', 'me', 'is', 'very', 'important', 'in', 'case', 'something', 'happens', 'and', 'you', 'need', 'surgery', 'and', 'you', 'can', 'get', 'referrals', 'to', 'see', 'specialists', 'without', 'having', 'to', 'see', 'him', 'first', 'really', 'what', 'more', 'do', 'you', 'need', 'im', 'sitting', 'here', 'trying', 'to', 'think', 'of', 'any', 'complaints', 'i', 'have', 'about', 'him', 'but', 'im', 'really', 'drawing', 'a', 'blank']\n5) --- Post join:\ndr goldberg offers everything i look for in a general practitioner hes nice and easy to talk to without being patronizing hes always on time in seeing his patients hes affiliated with a topnotch hospital nyu which my parents have explained to me is very important in case something happens and you need surgery and you can get referrals to see specialists without having to see him first really what more do you need im sitting here trying to think of any complaints i have about him but im really drawing a blank\n\n\n\n\n1) --- Current text:\nUnfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.\n2) --- Post lower:\nunfortunately, the frustration of being dr. goldberg's patient is a repeat of the experience i've had with so many other doctors in nyc -- good doctor, terrible staff.  it seems that his staff simply never answers the phone.  it usually takes 2 hours of repeated calling to get an answer.  who has time for that or wants to deal with it?  i have run into this problem with many other doctors and i just don't get it.  you have office workers, you have patients with medical needs, why isn't anyone answering the phone?  it's incomprehensible and not work the aggravation.  it's with regret that i feel that i have to give dr. goldberg 2 stars.\n3) --- Post regex sub (remove non alpha numerical chars):\nunfortunately the frustration of being dr goldbergs patient is a repeat of the experience ive had with so many other doctors in nyc  good doctor terrible staff  it seems that his staff simply never answers the phone  it usually takes 2 hours of repeated calling to get an answer  who has time for that or wants to deal with it  i have run into this problem with many other doctors and i just dont get it  you have office workers you have patients with medical needs why isnt anyone answering the phone  its incomprehensible and not work the aggravation  its with regret that i feel that i have to give dr goldberg 2 stars\n4) --- Post tokenization:\n['unfortunately', 'the', 'frustration', 'of', 'being', 'dr', 'goldbergs', 'patient', 'is', 'a', 'repeat', 'of', 'the', 'experience', 'ive', 'had', 'with', 'so', 'many', 'other', 'doctors', 'in', 'nyc', 'good', 'doctor', 'terrible', 'staff', 'it', 'seems', 'that', 'his', 'staff', 'simply', 'never', 'answers', 'the', 'phone', 'it', 'usually', 'takes', '2', 'hours', 'of', 'repeated', 'calling', 'to', 'get', 'an', 'answer', 'who', 'has', 'time', 'for', 'that', 'or', 'wants', 'to', 'deal', 'with', 'it', 'i', 'have', 'run', 'into', 'this', 'problem', 'with', 'many', 'other', 'doctors', 'and', 'i', 'just', 'dont', 'get', 'it', 'you', 'have', 'office', 'workers', 'you', 'have', 'patients', 'with', 'medical', 'needs', 'why', 'isnt', 'anyone', 'answering', 'the', 'phone', 'its', 'incomprehensible', 'and', 'not', 'work', 'the', 'aggravation', 'its', 'with', 'regret', 'that', 'i', 'feel', 'that', 'i', 'have', 'to', 'give', 'dr', 'goldberg', '2', 'stars']\n5) --- Post join:\nunfortunately the frustration of being dr goldbergs patient is a repeat of the experience ive had with so many other doctors in nyc good doctor terrible staff it seems that his staff simply never answers the phone it usually takes 2 hours of repeated calling to get an answer who has time for that or wants to deal with it i have run into this problem with many other doctors and i just dont get it you have office workers you have patients with medical needs why isnt anyone answering the phone its incomprehensible and not work the aggravation its with regret that i feel that i have to give dr goldberg 2 stars\n\n\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# .apply takes a function, every row of column is given to it for processing.\ntrain_df['preprocessed_sentences'] = train_df['text'].apply(preprocessing)\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-14T13:56:30.592195Z","iopub.execute_input":"2024-04-14T13:56:30.592515Z","iopub.status.idle":"2024-04-14T13:57:45.116016Z","shell.execute_reply.started":"2024-04-14T13:56:30.592485Z","shell.execute_reply":"2024-04-14T13:57:45.115071Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"   label                                               text  \\\n0      5  dr. goldberg offers everything i look for in a...   \n1      2  Unfortunately, the frustration of being Dr. Go...   \n2      4  Been going to Dr. Goldberg for over 10 years. ...   \n3      4  Got a letter in the mail last week that said D...   \n4      1  I don't know what Dr. Goldberg was like before...   \n\n                              preprocessed_sentences  \n0  dr goldberg offers everything i look for in a ...  \n1  unfortunately the frustration of being dr gold...  \n2  been going to dr goldberg for over 10 years i ...  \n3  got a letter in the mail last week that said d...  \n4  i dont know what dr goldberg was like before m...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n      <th>preprocessed_sentences</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>dr. goldberg offers everything i look for in a...</td>\n      <td>dr goldberg offers everything i look for in a ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Unfortunately, the frustration of being Dr. Go...</td>\n      <td>unfortunately the frustration of being dr gold...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n      <td>been going to dr goldberg for over 10 years i ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Got a letter in the mail last week that said D...</td>\n      <td>got a letter in the mail last week that said d...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>I don't know what Dr. Goldberg was like before...</td>\n      <td>i dont know what dr goldberg was like before m...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Same with test df\ntest_df['preprocessed_sentences'] = test_df['text'].apply(preprocessing)\n\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-14T13:57:45.117472Z","iopub.execute_input":"2024-04-14T13:57:45.117903Z","iopub.status.idle":"2024-04-14T13:57:50.811199Z","shell.execute_reply.started":"2024-04-14T13:57:45.117870Z","shell.execute_reply":"2024-04-14T13:57:50.810264Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"   label                                               text  \\\n0      1  I got 'new' tires from them and within two wee...   \n1      1  Don't waste your time.  We had two different p...   \n2      1  All I can say is the worst! We were the only 2...   \n3      1  I have been to this restaurant twice and was d...   \n4      1  Food was NOT GOOD at all! My husband & I ate h...   \n\n                              preprocessed_sentences  \n0  i got new tires from them and within two weeks...  \n1  dont waste your time we had two different peop...  \n2  all i can say is the worst we were the only 2 ...  \n3  i have been to this restaurant twice and was d...  \n4  food was not good at all my husband i ate here...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n      <th>preprocessed_sentences</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>I got 'new' tires from them and within two wee...</td>\n      <td>i got new tires from them and within two weeks...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Don't waste your time.  We had two different p...</td>\n      <td>dont waste your time we had two different peop...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>All I can say is the worst! We were the only 2...</td>\n      <td>all i can say is the worst we were the only 2 ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>I have been to this restaurant twice and was d...</td>\n      <td>i have been to this restaurant twice and was d...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>Food was NOT GOOD at all! My husband &amp; I ate h...</td>\n      <td>food was not good at all my husband i ate here...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# 3) Splitting","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n''' In the colab ag news lstm project, I got beautiful results on train and valid acc and losses. I \n    suspected because the dataset was smaller, it'd do better because model wouldn't have too much \n    training data to even overfit on. \n\n    Lstm ag news project:\n       1) train size - 120k\n       2) test size - 7600\n       3) train test split used with 0.2 so 20% of train data will be validation data. \n       4) so train data is 96k, and test is 24k\n\n    Yelp project\n       1) train size - 650k\n       2) test size - 50k\n       3) train test split used with 0.2 so 20% of train data will be validation data. \n       4) so train data is 520k, and test is 130k. \n\n    Point is, in first one, there's a 72k difference between train and valid set. The \n    second has a 390k difference! That's SO much more massive. So maybe setting test size \n    to half would help.\n    \n        ***New test_size values***\n        1) 0.45 - Train will be 357,500. Test will be 292,500. That's a 64,500 difference. MUCH\n            better than the last difference of 130k.\n'''\nsplit_test_size = 0.45\n\nx_train, x_val, y_train, y_val = train_test_split(train_df['preprocessed_sentences'],\n                                                  train_df['label'],\n                                                  test_size=split_test_size) \n# Display a few\nnum_to_display = 3\nprint(f'x_train:\\n{x_train[:num_to_display]}\\n\\ny_train:\\n{y_train[:num_to_display]}')","metadata":{"execution":{"iopub.status.busy":"2024-04-14T13:57:50.817052Z","iopub.execute_input":"2024-04-14T13:57:50.817366Z","iopub.status.idle":"2024-04-14T13:57:51.496447Z","shell.execute_reply.started":"2024-04-14T13:57:50.817340Z","shell.execute_reply":"2024-04-14T13:57:51.495512Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"x_train:\n93324     spent a few hours here saturday afternoon bowl...\n64881     andrew j and i grabbed boba here before drivin...\n277348    if youre on the strip and want a good burger t...\nName: preprocessed_sentences, dtype: object\n\ny_train:\n93324     3\n64881     4\n277348    3\nName: label, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"len(x_train), len(y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T13:57:51.497515Z","iopub.execute_input":"2024-04-14T13:57:51.497984Z","iopub.status.idle":"2024-04-14T13:57:51.504453Z","shell.execute_reply.started":"2024-04-14T13:57:51.497958Z","shell.execute_reply":"2024-04-14T13:57:51.503540Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"(357500, 357500)"},"metadata":{}}]},{"cell_type":"code","source":"len(x_val), len(y_val)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T13:57:51.506172Z","iopub.execute_input":"2024-04-14T13:57:51.506503Z","iopub.status.idle":"2024-04-14T13:57:51.515308Z","shell.execute_reply.started":"2024-04-14T13:57:51.506474Z","shell.execute_reply":"2024-04-14T13:57:51.514419Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"(292500, 292500)"},"metadata":{}}]},{"cell_type":"code","source":"# Correct the indices\nx_train.reset_index(drop=True, inplace=True)\nx_val.reset_index(drop=True, inplace=True)\n\ny_train.reset_index(drop=True, inplace=True)\ny_val.reset_index(drop=True, inplace=True)\n\nprint(f'x_train:\\n{x_train.head()}\\n\\ny_train:\\n{y_train.head()}\\n\\n')\nprint(f'x_val:\\n{x_val.head()}\\n\\ny_val:\\n{y_val.head()}\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2024-04-14T13:57:51.516988Z","iopub.execute_input":"2024-04-14T13:57:51.517318Z","iopub.status.idle":"2024-04-14T13:57:51.528224Z","shell.execute_reply.started":"2024-04-14T13:57:51.517289Z","shell.execute_reply":"2024-04-14T13:57:51.527214Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"x_train:\n0    spent a few hours here saturday afternoon bowl...\n1    andrew j and i grabbed boba here before drivin...\n2    if youre on the strip and want a good burger t...\n3    i loveloveloved this hotel i did a site visit ...\n4    the food was way overpriced in my opinion but ...\nName: preprocessed_sentences, dtype: object\n\ny_train:\n0    3\n1    4\n2    3\n3    5\n4    2\nName: label, dtype: int64\n\n\nx_val:\n0    i have been to the dd many times they host the...\n1    not the best ive had the service is not good t...\n2    i was walking to lucilles bbq to celebrate a f...\n3    we stumbled across this new restaurantlounge t...\n4    gas scam nif you do not have gas receipt even ...\nName: preprocessed_sentences, dtype: object\n\ny_val:\n0    4\n1    2\n2    5\n3    4\n4    1\nName: label, dtype: int64\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 4) Tokenize training text","metadata":{}},{"cell_type":"code","source":"x_train_tokens = [tk(sent) for sent in x_train]\nx_val_tokens = [tk(sent) for sent in x_val]\n\n# Display a few\nprint(f'{num_to_display} x_train sentences:\\n{x_train[:num_to_display]}\\n')\nprint(f'THEN {num_to_display} x_train_tokens sentences:\\n{x_train_tokens[:num_to_display]}\\n\\n\\n')\n\nprint(f'{num_to_display} x_val sentences:\\n{x_val[:num_to_display]}\\n')\nprint(f'THEN {num_to_display} x_val_tokens sentences:\\n{x_val_tokens[:num_to_display]}')","metadata":{"execution":{"iopub.status.busy":"2024-04-14T13:57:51.529497Z","iopub.execute_input":"2024-04-14T13:57:51.529865Z","iopub.status.idle":"2024-04-14T13:58:49.669710Z","shell.execute_reply.started":"2024-04-14T13:57:51.529840Z","shell.execute_reply":"2024-04-14T13:58:49.668751Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"3 x_train sentences:\n0    spent a few hours here saturday afternoon bowl...\n1    andrew j and i grabbed boba here before drivin...\n2    if youre on the strip and want a good burger t...\nName: preprocessed_sentences, dtype: object\n\nTHEN 3 x_train_tokens sentences:\n[['spent', 'a', 'few', 'hours', 'here', 'saturday', 'afternoon', 'bowling', 'and', 'playing', 'video', 'games', 'with', 'my', 'little', 'bro', 'and', 'im', 'going', 'to', 'try', 'to', 'not', 'let', 'the', 'fact', 'that', 'i', 'bowled', '5', 'strikes', 'in', 'my', 'best', 'game', 'ever', 'affect', 'my', 'review', 'of', 'the', 'place', '5', 'strikes', 'unbelievable', 'my', 'little', 'bro', 'was', 'so', 'impressed', 'i', 'didnt', 'have', 'the', 'heart', 'to', 'tell', 'him', 'i', 'normally', 'suck', 'nngoodnservice', 'was', 'friendly', 'and', 'helpful', 'had', 'several', 'folks', 'stop', 'by', 'while', 'we', 'were', 'bowling', 'just', 'to', 'see', 'how', 'we', 'were', 'doing', 'and', 'if', 'we', 'needed', 'anything', 'also', 'lost', 'some', 'quarters', 'in', 'a', 'video', 'game', 'that', 'wasnt', 'working', 'and', 'were', 'refunded', 'quickly', 'and', 'without', 'questionnfamily', 'friendly', 'environment', 'several', 'other', 'groups', 'with', 'kidsnnbadnlimited', 'ball', 'selection', 'im', 'not', 'an', 'expert', 'but', 'im', 'pretty', 'sure', 'my', 'fingers', 'are', 'regular', 'guy', 'size', 'fingers', 'and', 'it', 'took', 'me', 'a', 'good', 'ten', 'minutes', 'to', 'track', 'down', 'a', 'ball', 'with', 'large', 'enough', 'holesna', 'little', 'pricy', 'for', 'bowling', 'i', 'thought', 'shoes', 'for', 'two', 'and', 'a', 'couple', 'games', 'ran', 'us', 'close', 'to', '30', 'bucks'], ['andrew', 'j', 'and', 'i', 'grabbed', 'boba', 'here', 'before', 'driving', 'back', 'to', 'the', 'oc', 'this', 'was', 'good', 'stuff', 'i', 'must', 'admit', 'i', 'was', 'a', 'skeptici', 'almost', 'laughed', 'at', 'the', 'idea', 'of', 'getting', 'boba', 'in', 'lv', 'at', 'firstbut', 'now', 'i', 'know', 'the', 'pearls', 'were', 'qq', 'rather', 'than', 'soft', 'and', 'mushy', 'and', 'the', 'tea', 'regular', 'milk', 'tea', 'was', 'flavorful', 'with', 'the', 'perfect', 'amount', 'of', 'sweetness', 'and', 'creaminess', 'ill', 'be', 'stopping', 'here', 'each', 'time', 'i', 'visit', 'lv', 'now'], ['if', 'youre', 'on', 'the', 'strip', 'and', 'want', 'a', 'good', 'burger', 'this', 'is', 'the', 'place', 'to', 'go', 'its', 'in', 'a', 'convenient', 'location', 'and', 'it', 'doesnt', 'take', 'too', 'long', 'to', 'be', 'seated', 'and', 'get', 'food', 'the', 'burgers', 'were', 'really', 'good', 'but', 'nothing', 'particularly', 'special', 'about', 'them']]\n\n\n\n3 x_val sentences:\n0    i have been to the dd many times they host the...\n1    not the best ive had the service is not good t...\n2    i was walking to lucilles bbq to celebrate a f...\nName: preprocessed_sentences, dtype: object\n\nTHEN 3 x_val_tokens sentences:\n[['i', 'have', 'been', 'to', 'the', 'dd', 'many', 'times', 'they', 'host', 'the', 'annual', 'derby', 'wedding', 'during', 'rollercon', 'which', 'is', 'truly', 'a', 'sight', 'to', 'beholdni', 'love', 'that', 'they', 'have', 'pool', 'tables', 'and', 'typical', 'garage', 'type', 'bands', 'herenit', 'can', 'get', 'very', 'loud', 'and', 'very', 'crowded', 'but', 'that', 'just', 'makes', 'it', 'that', 'much', 'more', 'funnnever', 'tried', 'the', 'bacon', 'martini', 'but', 'have', 'tried', 'the', 'ass', 'juice', 'and', 'its', 'tastes', 'like', 'it', 'soundsbut', 'its', 'their', 'trademark', 'so', 'you', 'gotta', 'indulge', 'it', 'wont', 'kill', 'ya', 'but', 'you', 'will', 'feel', 'as', 'if', 'your', 'dyingncheck', 'out', 'the', 'girls', 'bathrooma', 'huge', 'mural', 'dedicated', 'to', 'the', 'derby', 'girlslove', 'thatncheap', 'drinks', 'musicpool', 'tables', 'love', 'that', 'the', 'most'], ['not', 'the', 'best', 'ive', 'had', 'the', 'service', 'is', 'not', 'good', 'the', 'food', 'is', 'so', 'so', 'and', 'the', 'pricing', 'seems', 'to', 'be', 'a', 'bit', 'high', 'for', 'the', 'quality'], ['i', 'was', 'walking', 'to', 'lucilles', 'bbq', 'to', 'celebrate', 'a', 'friends', 'birthday', 'when', 'the', 'smell', 'of', 'buffalo', 'wings', 'wafted', 'into', 'my', 'nostrils', 'with', 'my', 'head', 'on', 'a', 'swivel', 'i', 'began', 'exploring', 'where', 'this', 'delicious', 'smell', 'was', 'emanating', 'fromnni', 'saw', 'outdoor', 'seating', 'a', 'sweet', 'looking', 'bar', 'some', 'good', 'looking', 'girls', 'tending', 'that', 'bar', 'and', 'buffalo', 'wingsnni', 'went', 'in', 'got', 'myself', 'a', 'sam', 'adams', 'ordered', 'myself', 'some', 'wings', 'and', 'then', 'made', 'the', 'mistake', 'of', 'also', 'ordering', 'myself', 'an', 'enormous', 'burger', 'i', 'was', 'able', 'to', 'customize', 'my', 'toppings', 'and', 'the', 'burger', 'reminded', 'me', 'of', 'a', 'better', 'fatburger', 'the', 'wings', 'were', 'great', 'the', 'beer', 'was', 'good', 'the', 'burger', 'was', 'solid', 'the', 'fries', 'were', 'pretty', 'delicious', 'nni', 'loved', 'this', 'place', 'is', 'perfect', 'for', 'groups', 'or', 'a', 'casual', 'date']]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 5) Building vocab","metadata":{}},{"cell_type":"code","source":"from torchtext.vocab import build_vocab_from_iterator\n\ndef get_tokenized_sentences(sentences):\n    for sent in sentences:\n        yield tk(sent)\n\n# Get all preprocessed sentences into a list and create list for tokenized sentences\nall_sentences = train_df['preprocessed_sentences'].values.tolist()\n\n\n''' To build vocab in pytorch, use build_vocab_from_iterator (bvfi). It takes a function as an arg, and \n    that function takes an interable. Function must use yield and yield will pause every iteration to \n    continue from where it left off. \n    \n    \"unk\" key is for when the vocab gets unknown words. When the vocab is built, it will KNOW, n amount\n    of words. However if a new word comes in and it's not familar with it, it'll be classified as \n    unk for unknown. '''\n\nvocab = build_vocab_from_iterator(get_tokenized_sentences(all_sentences), specials=[\"<unk>\"])\nvocab.set_default_index(vocab[\"<unk>\"])","metadata":{"execution":{"iopub.status.busy":"2024-04-14T13:58:49.670929Z","iopub.execute_input":"2024-04-14T13:58:49.671239Z","iopub.status.idle":"2024-04-14T13:59:57.258239Z","shell.execute_reply.started":"2024-04-14T13:58:49.671215Z","shell.execute_reply":"2024-04-14T13:59:57.257197Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Get entire dictionary which contains key,value pairs of string/ints\nd = vocab.get_stoi()\n\n# Display a few. Each string with an assigned number.\nfor index, key_value in zip(range(5), d.items()):\n    print(f'Key: {key_value[0]}. Value: {key_value[1]}')","metadata":{"execution":{"iopub.status.busy":"2024-04-14T13:59:57.259437Z","iopub.execute_input":"2024-04-14T13:59:57.260157Z","iopub.status.idle":"2024-04-14T13:59:58.318585Z","shell.execute_reply.started":"2024-04-14T13:59:57.260119Z","shell.execute_reply":"2024-04-14T13:59:58.317364Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Key: zzzzzzzzzzzznnbut. Value: 837409\nKey: zzzzzzzzznnmy. Value: 837405\nKey: zzzzzzzzznclick. Value: 837404\nKey: zzzzzzzznnwake. Value: 837403\nKey: zzzzzzzznn45. Value: 837402\n","output_type":"stream"}]},{"cell_type":"code","source":"# Can now convert whole sentences into nums. Example below.\ndef convert_sent_to_nums(sent, print_steps=False):\n    if print_steps is True:\n        print(f'Sentence (as it is):\\n{sent}\\n')\n    return vocab(tk(sent))\n\nc = convert_sent_to_nums(train_df['preprocessed_sentences'][0], True)\nprint(f'Converted sentence:\\n{c}')","metadata":{"execution":{"iopub.status.busy":"2024-04-14T13:59:58.320739Z","iopub.execute_input":"2024-04-14T13:59:58.321083Z","iopub.status.idle":"2024-04-14T13:59:58.333171Z","shell.execute_reply.started":"2024-04-14T13:59:58.321056Z","shell.execute_reply":"2024-04-14T13:59:58.332144Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Sentence (as it is):\ndr goldberg offers everything i look for in a general practitioner hes nice and easy to talk to without being patronizing hes always on time in seeing his patients hes affiliated with a topnotch hospital nyu which my parents have explained to me is very important in case something happens and you need surgery and you can get referrals to see specialists without having to see him first really what more do you need im sitting here trying to think of any complaints i have about him but im really drawing a blank\n\nConverted sentence:\n[1046, 46721, 1378, 192, 4, 257, 9, 10, 3, 1108, 14001, 1338, 85, 2, 613, 5, 731, 5, 325, 165, 9640, 1338, 126, 20, 50, 10, 976, 187, 3502, 1338, 10462, 17, 3, 5955, 2437, 204992, 63, 13, 1617, 22, 1071, 5, 34, 11, 40, 1483, 10, 805, 166, 1750, 2, 19, 222, 3574, 2, 19, 74, 42, 14061, 5, 145, 13641, 325, 334, 5, 145, 251, 104, 59, 61, 68, 82, 19, 222, 86, 528, 43, 356, 5, 121, 7, 130, 1445, 4, 22, 55, 251, 14, 86, 59, 8526, 3, 5671]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Loop up the above values just to double check\n[vocab.get_itos()[num] for num in c]","metadata":{"execution":{"iopub.status.busy":"2024-04-14T13:59:58.334691Z","iopub.execute_input":"2024-04-14T13:59:58.335150Z","iopub.status.idle":"2024-04-14T14:00:09.606101Z","shell.execute_reply.started":"2024-04-14T13:59:58.335103Z","shell.execute_reply":"2024-04-14T14:00:09.605131Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"['dr',\n 'goldberg',\n 'offers',\n 'everything',\n 'i',\n 'look',\n 'for',\n 'in',\n 'a',\n 'general',\n 'practitioner',\n 'hes',\n 'nice',\n 'and',\n 'easy',\n 'to',\n 'talk',\n 'to',\n 'without',\n 'being',\n 'patronizing',\n 'hes',\n 'always',\n 'on',\n 'time',\n 'in',\n 'seeing',\n 'his',\n 'patients',\n 'hes',\n 'affiliated',\n 'with',\n 'a',\n 'topnotch',\n 'hospital',\n 'nyu',\n 'which',\n 'my',\n 'parents',\n 'have',\n 'explained',\n 'to',\n 'me',\n 'is',\n 'very',\n 'important',\n 'in',\n 'case',\n 'something',\n 'happens',\n 'and',\n 'you',\n 'need',\n 'surgery',\n 'and',\n 'you',\n 'can',\n 'get',\n 'referrals',\n 'to',\n 'see',\n 'specialists',\n 'without',\n 'having',\n 'to',\n 'see',\n 'him',\n 'first',\n 'really',\n 'what',\n 'more',\n 'do',\n 'you',\n 'need',\n 'im',\n 'sitting',\n 'here',\n 'trying',\n 'to',\n 'think',\n 'of',\n 'any',\n 'complaints',\n 'i',\n 'have',\n 'about',\n 'him',\n 'but',\n 'im',\n 'really',\n 'drawing',\n 'a',\n 'blank']"},"metadata":{}}]},{"cell_type":"code","source":"# Now convert all preprocessed strings and display a few\nx_train_sequences = [vocab(tk(text)) for text in x_train]\nx_val_sequences = [vocab(tk(text)) for text in x_val]\n\nprint(f'x_train_sequences:\\n{x_train_sequences[:num_to_display]}\\n')\nprint(f'x_val_sequences:\\n{x_val_sequences[:num_to_display]}\\n')","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:00:09.607242Z","iopub.execute_input":"2024-04-14T14:00:09.607512Z","iopub.status.idle":"2024-04-14T14:01:17.810294Z","shell.execute_reply.started":"2024-04-14T14:00:09.607490Z","shell.execute_reply":"2024-04-14T14:01:17.809209Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"x_train_sequences:\n[[743, 3, 147, 379, 43, 582, 988, 2965, 2, 931, 2094, 1096, 17, 13, 97, 7374, 2, 86, 114, 5, 112, 5, 21, 327, 1, 466, 12, 4, 15040, 202, 6500, 10, 13, 109, 704, 163, 7449, 13, 296, 7, 1, 31, 202, 6500, 3421, 13, 97, 7374, 6, 26, 559, 4, 80, 22, 1, 1380, 5, 347, 251, 4, 1018, 2301, 591609, 6, 159, 2, 557, 23, 410, 1214, 471, 75, 156, 15, 24, 2965, 36, 5, 145, 117, 15, 24, 533, 2, 37, 15, 412, 247, 81, 889, 62, 5662, 10, 3, 2094, 704, 12, 142, 519, 2, 24, 4859, 598, 2, 325, 663419, 159, 1502, 410, 78, 1808, 17, 524549, 2010, 308, 86, 21, 58, 3973, 14, 86, 102, 162, 13, 2093, 27, 538, 375, 514, 2093, 2, 8, 160, 34, 3, 30, 1387, 137, 5, 2673, 139, 3, 2010, 17, 309, 218, 491348, 97, 3710, 9, 2965, 4, 236, 1449, 9, 120, 2, 3, 320, 1096, 1278, 71, 394, 5, 493, 901], [8173, 5172, 2, 4, 2172, 1981, 43, 132, 1354, 56, 5, 1, 10691, 16, 6, 30, 492, 4, 405, 1492, 4, 6, 3, 718813, 321, 3275, 25, 1, 705, 7, 276, 1981, 10, 2240, 25, 62949, 170, 4, 108, 1, 11414, 24, 28182, 490, 88, 860, 2, 2260, 2, 1, 510, 538, 1304, 510, 6, 855, 17, 1, 403, 548, 7, 3091, 2, 10550, 318, 29, 2723, 43, 302, 50, 4, 279, 2240, 170], [37, 210, 20, 1, 315, 2, 131, 3, 30, 213, 16, 11, 1, 31, 5, 57, 45, 10, 3, 1196, 196, 2, 8, 370, 151, 84, 190, 5, 29, 450, 2, 42, 28, 1, 570, 24, 59, 30, 14, 189, 1467, 269, 55, 79]]\n\nx_val_sequences:\n[[4, 22, 66, 5, 1, 6466, 182, 172, 18, 1577, 1, 5055, 12721, 1251, 368, 57113, 63, 11, 970, 3, 2844, 5, 305601, 123, 12, 18, 22, 486, 348, 2, 856, 1986, 657, 3077, 42279, 74, 42, 40, 678, 2, 40, 774, 14, 12, 36, 460, 8, 12, 92, 68, 456595, 214, 1, 535, 2022, 14, 22, 214, 1, 1473, 1198, 2, 45, 960, 35, 8, 728218, 45, 54, 15478, 26, 19, 1901, 5107, 8, 346, 2502, 2463, 14, 19, 69, 235, 33, 37, 67, 409166, 38, 1, 690, 122526, 307, 10053, 4697, 5, 1, 12721, 184346, 763649, 194, 577694, 348, 123, 12, 1, 171], [21, 1, 109, 105, 23, 1, 53, 11, 21, 30, 1, 28, 11, 26, 26, 2, 1, 1496, 463, 5, 29, 3, 176, 339, 9, 1, 246], [4, 6, 633, 5, 7943, 554, 5, 2700, 3, 243, 718, 47, 1, 989, 7, 1682, 573, 22935, 174, 13, 18247, 17, 13, 701, 20, 3, 18398, 4, 1956, 6762, 177, 16, 200, 989, 6, 19198, 25034, 430, 1170, 592, 3, 294, 221, 127, 62, 30, 221, 690, 9345, 12, 127, 2, 1682, 38387, 98, 10, 76, 494, 3, 4376, 8759, 87, 494, 62, 573, 2, 101, 135, 1, 1103, 7, 81, 659, 494, 58, 3812, 213, 4, 6, 454, 5, 6335, 13, 956, 2, 1, 213, 1633, 34, 7, 3, 106, 7541, 1, 573, 24, 52, 1, 273, 6, 30, 1, 213, 6, 1034, 1, 237, 24, 102, 200, 233, 445, 16, 31, 11, 403, 9, 1808, 46, 3, 1352, 890]]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Show example of original text in one of the x_train_sequences by converting\ndef convert_num_to_sentence(nums, print_steps=False):\n    if print_steps is True:\n        print(f'NUMERICAL sentence (as it is):\\n{nums}\\n')\n    \n    # get_itos gets a dictionary of int and str pairs. Pass num to get str equal\n    s = [vocab.get_itos()[num] for num in nums]\n    \n    # Then join for original string\n    return ' '.join(s)\n\nconvert_num_to_sentence(x_train_sequences[0], True)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:01:17.811350Z","iopub.execute_input":"2024-04-14T14:01:17.811627Z","iopub.status.idle":"2024-04-14T14:01:39.027985Z","shell.execute_reply.started":"2024-04-14T14:01:17.811604Z","shell.execute_reply":"2024-04-14T14:01:39.027011Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"NUMERICAL sentence (as it is):\n[743, 3, 147, 379, 43, 582, 988, 2965, 2, 931, 2094, 1096, 17, 13, 97, 7374, 2, 86, 114, 5, 112, 5, 21, 327, 1, 466, 12, 4, 15040, 202, 6500, 10, 13, 109, 704, 163, 7449, 13, 296, 7, 1, 31, 202, 6500, 3421, 13, 97, 7374, 6, 26, 559, 4, 80, 22, 1, 1380, 5, 347, 251, 4, 1018, 2301, 591609, 6, 159, 2, 557, 23, 410, 1214, 471, 75, 156, 15, 24, 2965, 36, 5, 145, 117, 15, 24, 533, 2, 37, 15, 412, 247, 81, 889, 62, 5662, 10, 3, 2094, 704, 12, 142, 519, 2, 24, 4859, 598, 2, 325, 663419, 159, 1502, 410, 78, 1808, 17, 524549, 2010, 308, 86, 21, 58, 3973, 14, 86, 102, 162, 13, 2093, 27, 538, 375, 514, 2093, 2, 8, 160, 34, 3, 30, 1387, 137, 5, 2673, 139, 3, 2010, 17, 309, 218, 491348, 97, 3710, 9, 2965, 4, 236, 1449, 9, 120, 2, 3, 320, 1096, 1278, 71, 394, 5, 493, 901]\n\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"'spent a few hours here saturday afternoon bowling and playing video games with my little bro and im going to try to not let the fact that i bowled 5 strikes in my best game ever affect my review of the place 5 strikes unbelievable my little bro was so impressed i didnt have the heart to tell him i normally suck nngoodnservice was friendly and helpful had several folks stop by while we were bowling just to see how we were doing and if we needed anything also lost some quarters in a video game that wasnt working and were refunded quickly and without questionnfamily friendly environment several other groups with kidsnnbadnlimited ball selection im not an expert but im pretty sure my fingers are regular guy size fingers and it took me a good ten minutes to track down a ball with large enough holesna little pricy for bowling i thought shoes for two and a couple games ran us close to 30 bucks'"},"metadata":{}}]},{"cell_type":"markdown","source":"# 6) Padding","metadata":{}},{"cell_type":"code","source":"import torch\n# Padding takes num sentences of different lengths into the same length.\n\n# test pad value\nmax_len = 5 \n\n# Get an example num sentence\ntest_num_sent = x_train_sequences[0]\nprint(f'Current numerical sentence:\\n{test_num_sent}\\nLength: {len(test_num_sent)}\\n\\n')\n\n# Convert to tensor because .pad func in torch requires tensors\ntest_num_sent = torch.tensor(test_num_sent)\nprint(f'Current numerical sentence as tensor:\\n{test_num_sent}\\n\\n')\n\n''' \"(0, max_len - len(test_num_sent))\" - Does the math. 2 classes\n\n    1) Length of sentence shorter than padded value - Ex: If sent is 20 nums long and\n        padding value/max_len is 50, 30 0s is added. That's where the \"0\" value comes into \n        play in the arguments. \"max_len - len(test_num_sent)\" is 50 - 20 = 30, so 30 0s added.\n        \n    2) Length of sentence LONGER than padded value - Ex: If sent is 17 nums long and padding value/\n        max_len is 5. The opposite happens. The sentence is chopped off or truncated to get to \n        max len of 5. \"max_len - len(test_num_sent)\" is 5 - 17 = -12 '''\n\ntest_num_sent = torch.nn.functional.pad(test_num_sent, (0, max_len - len(test_num_sent)), mode='constant', value=0)\nprint(f'Padded numerical sentence as tensor:\\n{test_num_sent}\\nNew length: {len(test_num_sent)}')","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:01:39.029403Z","iopub.execute_input":"2024-04-14T14:01:39.029827Z","iopub.status.idle":"2024-04-14T14:01:39.092363Z","shell.execute_reply.started":"2024-04-14T14:01:39.029791Z","shell.execute_reply":"2024-04-14T14:01:39.091473Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Current numerical sentence:\n[743, 3, 147, 379, 43, 582, 988, 2965, 2, 931, 2094, 1096, 17, 13, 97, 7374, 2, 86, 114, 5, 112, 5, 21, 327, 1, 466, 12, 4, 15040, 202, 6500, 10, 13, 109, 704, 163, 7449, 13, 296, 7, 1, 31, 202, 6500, 3421, 13, 97, 7374, 6, 26, 559, 4, 80, 22, 1, 1380, 5, 347, 251, 4, 1018, 2301, 591609, 6, 159, 2, 557, 23, 410, 1214, 471, 75, 156, 15, 24, 2965, 36, 5, 145, 117, 15, 24, 533, 2, 37, 15, 412, 247, 81, 889, 62, 5662, 10, 3, 2094, 704, 12, 142, 519, 2, 24, 4859, 598, 2, 325, 663419, 159, 1502, 410, 78, 1808, 17, 524549, 2010, 308, 86, 21, 58, 3973, 14, 86, 102, 162, 13, 2093, 27, 538, 375, 514, 2093, 2, 8, 160, 34, 3, 30, 1387, 137, 5, 2673, 139, 3, 2010, 17, 309, 218, 491348, 97, 3710, 9, 2965, 4, 236, 1449, 9, 120, 2, 3, 320, 1096, 1278, 71, 394, 5, 493, 901]\nLength: 166\n\n\nCurrent numerical sentence as tensor:\ntensor([   743,      3,    147,    379,     43,    582,    988,   2965,      2,\n           931,   2094,   1096,     17,     13,     97,   7374,      2,     86,\n           114,      5,    112,      5,     21,    327,      1,    466,     12,\n             4,  15040,    202,   6500,     10,     13,    109,    704,    163,\n          7449,     13,    296,      7,      1,     31,    202,   6500,   3421,\n            13,     97,   7374,      6,     26,    559,      4,     80,     22,\n             1,   1380,      5,    347,    251,      4,   1018,   2301, 591609,\n             6,    159,      2,    557,     23,    410,   1214,    471,     75,\n           156,     15,     24,   2965,     36,      5,    145,    117,     15,\n            24,    533,      2,     37,     15,    412,    247,     81,    889,\n            62,   5662,     10,      3,   2094,    704,     12,    142,    519,\n             2,     24,   4859,    598,      2,    325, 663419,    159,   1502,\n           410,     78,   1808,     17, 524549,   2010,    308,     86,     21,\n            58,   3973,     14,     86,    102,    162,     13,   2093,     27,\n           538,    375,    514,   2093,      2,      8,    160,     34,      3,\n            30,   1387,    137,      5,   2673,    139,      3,   2010,     17,\n           309,    218, 491348,     97,   3710,      9,   2965,      4,    236,\n          1449,      9,    120,      2,      3,    320,   1096,   1278,     71,\n           394,      5,    493,    901])\n\n\nPadded numerical sentence as tensor:\ntensor([743,   3, 147, 379,  43])\nNew length: 5\n","output_type":"stream"}]},{"cell_type":"code","source":"# Out of curiosity what is the max length of numerical sentences? \nmax_num = max(len(x) for x in x_train_sequences)\n\nmax_num","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:01:39.093624Z","iopub.execute_input":"2024-04-14T14:01:39.093915Z","iopub.status.idle":"2024-04-14T14:01:39.140779Z","shell.execute_reply.started":"2024-04-14T14:01:39.093891Z","shell.execute_reply":"2024-04-14T14:01:39.139944Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"1025"},"metadata":{}}]},{"cell_type":"code","source":"# Get length of all num lists for graphing soon\nx_train_lengths = [len(list_of_nums) for list_of_nums in x_train_sequences]","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:01:39.142020Z","iopub.execute_input":"2024-04-14T14:01:39.142352Z","iopub.status.idle":"2024-04-14T14:01:39.184107Z","shell.execute_reply.started":"2024-04-14T14:01:39.142322Z","shell.execute_reply":"2024-04-14T14:01:39.183146Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"series_lengths = pd.Series(data=x_train_lengths)\n\nseries_lengths","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:01:39.185302Z","iopub.execute_input":"2024-04-14T14:01:39.185609Z","iopub.status.idle":"2024-04-14T14:01:39.348000Z","shell.execute_reply.started":"2024-04-14T14:01:39.185578Z","shell.execute_reply":"2024-04-14T14:01:39.347076Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"0         166\n1          74\n2          44\n3         215\n4          22\n         ... \n357495    340\n357496    337\n357497    213\n357498     95\n357499    271\nLength: 357500, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"''' This shows padding to the max length probably isn't best. A few cells ago, the max len was 1033. \n    ALL other num sentences will have to be padded to that length. A waste of space and more processing\n    time most likely.\n    \n    Padding to max len of 1033 will NOT be done, a smaller value will be chosen. '''\nseries_lengths.hist()","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:01:39.349370Z","iopub.execute_input":"2024-04-14T14:01:39.349655Z","iopub.status.idle":"2024-04-14T14:01:39.693656Z","shell.execute_reply.started":"2024-04-14T14:01:39.349624Z","shell.execute_reply":"2024-04-14T14:01:39.692805Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"<Axes: >"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"# Use same num sentence and pad to greater length as an example\ntest_num_sent = torch.tensor(x_train_sequences[0])\n\n# \"(len(test_num_sent) * 2) - len(test_num_sent)\" Guarantees we'll have to pad and add 0s. '''\n\na = torch.nn.functional.pad(test_num_sent, \n                            (0, (len(test_num_sent) * 2) - len(test_num_sent)), \n                            mode='constant', \n                            value=0)\n\nprint(f'Test_num_sent:\\n{test_num_sent}\\nLength: {len(test_num_sent)}\\nNew padded sent:\\n{a}\\nLength: {len(a)}')","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:01:39.695046Z","iopub.execute_input":"2024-04-14T14:01:39.695373Z","iopub.status.idle":"2024-04-14T14:01:39.710665Z","shell.execute_reply.started":"2024-04-14T14:01:39.695341Z","shell.execute_reply":"2024-04-14T14:01:39.709707Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Test_num_sent:\ntensor([   743,      3,    147,    379,     43,    582,    988,   2965,      2,\n           931,   2094,   1096,     17,     13,     97,   7374,      2,     86,\n           114,      5,    112,      5,     21,    327,      1,    466,     12,\n             4,  15040,    202,   6500,     10,     13,    109,    704,    163,\n          7449,     13,    296,      7,      1,     31,    202,   6500,   3421,\n            13,     97,   7374,      6,     26,    559,      4,     80,     22,\n             1,   1380,      5,    347,    251,      4,   1018,   2301, 591609,\n             6,    159,      2,    557,     23,    410,   1214,    471,     75,\n           156,     15,     24,   2965,     36,      5,    145,    117,     15,\n            24,    533,      2,     37,     15,    412,    247,     81,    889,\n            62,   5662,     10,      3,   2094,    704,     12,    142,    519,\n             2,     24,   4859,    598,      2,    325, 663419,    159,   1502,\n           410,     78,   1808,     17, 524549,   2010,    308,     86,     21,\n            58,   3973,     14,     86,    102,    162,     13,   2093,     27,\n           538,    375,    514,   2093,      2,      8,    160,     34,      3,\n            30,   1387,    137,      5,   2673,    139,      3,   2010,     17,\n           309,    218, 491348,     97,   3710,      9,   2965,      4,    236,\n          1449,      9,    120,      2,      3,    320,   1096,   1278,     71,\n           394,      5,    493,    901])\nLength: 166\nNew padded sent:\ntensor([   743,      3,    147,    379,     43,    582,    988,   2965,      2,\n           931,   2094,   1096,     17,     13,     97,   7374,      2,     86,\n           114,      5,    112,      5,     21,    327,      1,    466,     12,\n             4,  15040,    202,   6500,     10,     13,    109,    704,    163,\n          7449,     13,    296,      7,      1,     31,    202,   6500,   3421,\n            13,     97,   7374,      6,     26,    559,      4,     80,     22,\n             1,   1380,      5,    347,    251,      4,   1018,   2301, 591609,\n             6,    159,      2,    557,     23,    410,   1214,    471,     75,\n           156,     15,     24,   2965,     36,      5,    145,    117,     15,\n            24,    533,      2,     37,     15,    412,    247,     81,    889,\n            62,   5662,     10,      3,   2094,    704,     12,    142,    519,\n             2,     24,   4859,    598,      2,    325, 663419,    159,   1502,\n           410,     78,   1808,     17, 524549,   2010,    308,     86,     21,\n            58,   3973,     14,     86,    102,    162,     13,   2093,     27,\n           538,    375,    514,   2093,      2,      8,    160,     34,      3,\n            30,   1387,    137,      5,   2673,    139,      3,   2010,     17,\n           309,    218, 491348,     97,   3710,      9,   2965,      4,    236,\n          1449,      9,    120,      2,      3,    320,   1096,   1278,     71,\n           394,      5,    493,    901,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0])\nLength: 332\n","output_type":"stream"}]},{"cell_type":"code","source":"# Pad all numerical sentences\ndef pad_num_sentences(num_sent, max_pad_length):\n    # return torch.nn.functional.pad(torch.tensor(num_sent), (0, max_pad_length - len(num_sent)), mode='constant', value=0)\n    \n    # Type specification should solve error.\n    return torch.nn.functional.pad(torch.tensor(num_sent, dtype=torch.int64), (0, max_pad_length - len(num_sent)), mode='constant', value=0)\n\n# A few cells ago, decision was made not to use max of 1033, but a value closer to the range of most num sentences.\nmax_padding_len = 200\n\n# cs = current sequence\nx_train_padded = [pad_num_sentences(cs, max_padding_len) for cs in x_train_sequences]\nx_val_padded = [pad_num_sentences(cs, max_padding_len) for cs in x_val_sequences]\n\n# Display\nprint(f'x_train_padded:\\n{x_train_padded[:num_to_display]}\\n\\n')\nprint(f'x_val_padded:\\n{x_val_padded[:num_to_display]}\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:01:39.711966Z","iopub.execute_input":"2024-04-14T14:01:39.712496Z","iopub.status.idle":"2024-04-14T14:02:05.678663Z","shell.execute_reply.started":"2024-04-14T14:01:39.712465Z","shell.execute_reply":"2024-04-14T14:02:05.677745Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"x_train_padded:\n[tensor([   743,      3,    147,    379,     43,    582,    988,   2965,      2,\n           931,   2094,   1096,     17,     13,     97,   7374,      2,     86,\n           114,      5,    112,      5,     21,    327,      1,    466,     12,\n             4,  15040,    202,   6500,     10,     13,    109,    704,    163,\n          7449,     13,    296,      7,      1,     31,    202,   6500,   3421,\n            13,     97,   7374,      6,     26,    559,      4,     80,     22,\n             1,   1380,      5,    347,    251,      4,   1018,   2301, 591609,\n             6,    159,      2,    557,     23,    410,   1214,    471,     75,\n           156,     15,     24,   2965,     36,      5,    145,    117,     15,\n            24,    533,      2,     37,     15,    412,    247,     81,    889,\n            62,   5662,     10,      3,   2094,    704,     12,    142,    519,\n             2,     24,   4859,    598,      2,    325, 663419,    159,   1502,\n           410,     78,   1808,     17, 524549,   2010,    308,     86,     21,\n            58,   3973,     14,     86,    102,    162,     13,   2093,     27,\n           538,    375,    514,   2093,      2,      8,    160,     34,      3,\n            30,   1387,    137,      5,   2673,    139,      3,   2010,     17,\n           309,    218, 491348,     97,   3710,      9,   2965,      4,    236,\n          1449,      9,    120,      2,      3,    320,   1096,   1278,     71,\n           394,      5,    493,    901,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0]), tensor([  8173,   5172,      2,      4,   2172,   1981,     43,    132,   1354,\n            56,      5,      1,  10691,     16,      6,     30,    492,      4,\n           405,   1492,      4,      6,      3, 718813,    321,   3275,     25,\n             1,    705,      7,    276,   1981,     10,   2240,     25,  62949,\n           170,      4,    108,      1,  11414,     24,  28182,    490,     88,\n           860,      2,   2260,      2,      1,    510,    538,   1304,    510,\n             6,    855,     17,      1,    403,    548,      7,   3091,      2,\n         10550,    318,     29,   2723,     43,    302,     50,      4,    279,\n          2240,    170,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0]), tensor([  37,  210,   20,    1,  315,    2,  131,    3,   30,  213,   16,   11,\n           1,   31,    5,   57,   45,   10,    3, 1196,  196,    2,    8,  370,\n         151,   84,  190,    5,   29,  450,    2,   42,   28,    1,  570,   24,\n          59,   30,   14,  189, 1467,  269,   55,   79,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0])]\n\n\nx_val_padded:\n[tensor([     4,     22,     66,      5,      1,   6466,    182,    172,     18,\n          1577,      1,   5055,  12721,   1251,    368,  57113,     63,     11,\n           970,      3,   2844,      5, 305601,    123,     12,     18,     22,\n           486,    348,      2,    856,   1986,    657,   3077,  42279,     74,\n            42,     40,    678,      2,     40,    774,     14,     12,     36,\n           460,      8,     12,     92,     68, 456595,    214,      1,    535,\n          2022,     14,     22,    214,      1,   1473,   1198,      2,     45,\n           960,     35,      8, 728218,     45,     54,  15478,     26,     19,\n          1901,   5107,      8,    346,   2502,   2463,     14,     19,     69,\n           235,     33,     37,     67, 409166,     38,      1,    690, 122526,\n           307,  10053,   4697,      5,      1,  12721, 184346, 763649,    194,\n        577694,    348,    123,     12,      1,    171,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0]), tensor([  21,    1,  109,  105,   23,    1,   53,   11,   21,   30,    1,   28,\n          11,   26,   26,    2,    1, 1496,  463,    5,   29,    3,  176,  339,\n           9,    1,  246,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0]), tensor([    4,     6,   633,     5,  7943,   554,     5,  2700,     3,   243,\n          718,    47,     1,   989,     7,  1682,   573, 22935,   174,    13,\n        18247,    17,    13,   701,    20,     3, 18398,     4,  1956,  6762,\n          177,    16,   200,   989,     6, 19198, 25034,   430,  1170,   592,\n            3,   294,   221,   127,    62,    30,   221,   690,  9345,    12,\n          127,     2,  1682, 38387,    98,    10,    76,   494,     3,  4376,\n         8759,    87,   494,    62,   573,     2,   101,   135,     1,  1103,\n            7,    81,   659,   494,    58,  3812,   213,     4,     6,   454,\n            5,  6335,    13,   956,     2,     1,   213,  1633,    34,     7,\n            3,   106,  7541,     1,   573,    24,    52,     1,   273,     6,\n           30,     1,   213,     6,  1034,     1,   237,    24,   102,   200,\n          233,   445,    16,    31,    11,   403,     9,  1808,    46,     3,\n         1352,   890,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])]\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"''' Checking types again. Old error was \"Expected tensor for argument #1 'indices' to have one of the following scalar types: \n    Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)\". In previous cell, the old\n    pad line is commented out because that was used before, no type specification so I assume the function TRIED to assume\n    the type. Didn't work out.\n    \n    Loop shouldn't print anything if all is well. '''\n\nfor i, cur_padded_tensor in enumerate(x_train_padded):\n    if cur_padded_tensor.dtype != torch.int64:\n        print(f'Index {i} has non int64 type!')","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:02:05.680066Z","iopub.execute_input":"2024-04-14T14:02:05.680434Z","iopub.status.idle":"2024-04-14T14:02:05.883689Z","shell.execute_reply.started":"2024-04-14T14:02:05.680403Z","shell.execute_reply":"2024-04-14T14:02:05.882972Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"x_train_padded[0].dtype","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:02:05.884729Z","iopub.execute_input":"2024-04-14T14:02:05.885088Z","iopub.status.idle":"2024-04-14T14:02:05.891093Z","shell.execute_reply.started":"2024-04-14T14:02:05.885047Z","shell.execute_reply":"2024-04-14T14:02:05.890095Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"torch.int64"},"metadata":{}}]},{"cell_type":"markdown","source":"# 7) Changing labels","metadata":{}},{"cell_type":"code","source":"''' 1,2,3,4 in set(y_train.values). .values is a numpy array while y_train currently is a \n    series object. '''\ny_train_np = y_train.values\ny_val_np = y_val.values\n\ny_train_np, y_val_np","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:02:05.892373Z","iopub.execute_input":"2024-04-14T14:02:05.892697Z","iopub.status.idle":"2024-04-14T14:02:05.905914Z","shell.execute_reply.started":"2024-04-14T14:02:05.892667Z","shell.execute_reply":"2024-04-14T14:02:05.905035Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"(array([3, 4, 3, ..., 2, 3, 1]), array([4, 2, 5, ..., 1, 4, 5]))"},"metadata":{}}]},{"cell_type":"code","source":"# Wondering what the labels were in both\nset(y_train_np), set(y_val_np)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:02:05.907041Z","iopub.execute_input":"2024-04-14T14:02:05.907328Z","iopub.status.idle":"2024-04-14T14:02:06.036040Z","shell.execute_reply.started":"2024-04-14T14:02:05.907305Z","shell.execute_reply":"2024-04-14T14:02:06.034923Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"({1, 2, 3, 4, 5}, {1, 2, 3, 4, 5})"},"metadata":{}}]},{"cell_type":"code","source":"max_of_y = max(set(y_train_np))\n\nmax_of_y","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:02:06.037412Z","iopub.execute_input":"2024-04-14T14:02:06.037723Z","iopub.status.idle":"2024-04-14T14:02:06.106474Z","shell.execute_reply.started":"2024-04-14T14:02:06.037701Z","shell.execute_reply":"2024-04-14T14:02:06.105608Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"5"},"metadata":{}}]},{"cell_type":"code","source":"''' If labels 1,2,3,4,5, there's a way to index at 0. Just get the biggest value and set that to 0.\n    Ex: labels are 1,2,3,4,5. Get 5, set every value of 5 in a dataframe column/series to 0. '''\ny_train_np[y_train_np == max_of_y] = 0\ny_val_np[y_val_np == max_of_y] = 0\n\nset(y_train_np), set(y_val_np)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:02:06.107513Z","iopub.execute_input":"2024-04-14T14:02:06.107831Z","iopub.status.idle":"2024-04-14T14:02:06.234966Z","shell.execute_reply.started":"2024-04-14T14:02:06.107809Z","shell.execute_reply":"2024-04-14T14:02:06.234055Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"({0, 1, 2, 3, 4}, {0, 1, 2, 3, 4})"},"metadata":{}}]},{"cell_type":"markdown","source":"# 8) Dataset & DataLoaders","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\n# Datasets make managing data easier in pytorch, especially for image projects. \n\nclass TextDataset(Dataset):\n    def __init__(self, padded_sentences, labels):\n        self.padded_sentences = padded_sentences\n        self.labels = labels\n        \n    def __len__(self):\n        return len(self.padded_sentences)\n    \n    def __getitem__(self, index):\n        return self.padded_sentences[index], self.labels[index]\n    \ntrain_data = TextDataset(x_train_padded, y_train_np)\nval_data = TextDataset(x_val_padded, y_val_np)\n\n# Test\nfor i in train_data:\n    print(i)\n    print(i[0].dtype)\n    break","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:02:06.235976Z","iopub.execute_input":"2024-04-14T14:02:06.236235Z","iopub.status.idle":"2024-04-14T14:02:06.245331Z","shell.execute_reply.started":"2024-04-14T14:02:06.236213Z","shell.execute_reply":"2024-04-14T14:02:06.244482Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"(tensor([   743,      3,    147,    379,     43,    582,    988,   2965,      2,\n           931,   2094,   1096,     17,     13,     97,   7374,      2,     86,\n           114,      5,    112,      5,     21,    327,      1,    466,     12,\n             4,  15040,    202,   6500,     10,     13,    109,    704,    163,\n          7449,     13,    296,      7,      1,     31,    202,   6500,   3421,\n            13,     97,   7374,      6,     26,    559,      4,     80,     22,\n             1,   1380,      5,    347,    251,      4,   1018,   2301, 591609,\n             6,    159,      2,    557,     23,    410,   1214,    471,     75,\n           156,     15,     24,   2965,     36,      5,    145,    117,     15,\n            24,    533,      2,     37,     15,    412,    247,     81,    889,\n            62,   5662,     10,      3,   2094,    704,     12,    142,    519,\n             2,     24,   4859,    598,      2,    325, 663419,    159,   1502,\n           410,     78,   1808,     17, 524549,   2010,    308,     86,     21,\n            58,   3973,     14,     86,    102,    162,     13,   2093,     27,\n           538,    375,    514,   2093,      2,      8,    160,     34,      3,\n            30,   1387,    137,      5,   2673,    139,      3,   2010,     17,\n           309,    218, 491348,     97,   3710,      9,   2965,      4,    236,\n          1449,      9,    120,      2,      3,    320,   1096,   1278,     71,\n           394,      5,    493,    901,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0]), 3)\ntorch.int64\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# DataLoaders handle batch sizes. When collate_fn arg is supplied, can handle preprocessing as well.\ntrain_loader = DataLoader(train_data, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=64, shuffle=True) ","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:02:06.246471Z","iopub.execute_input":"2024-04-14T14:02:06.246799Z","iopub.status.idle":"2024-04-14T14:02:06.255402Z","shell.execute_reply.started":"2024-04-14T14:02:06.246769Z","shell.execute_reply":"2024-04-14T14:02:06.254556Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"## 8.2) Understanding Embedding & LSTM","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\n''' Embedding layer maps each word in the vocab with a tensor of size n to represent it in the neural network. \n    Ex: Embedding dim is 125 (like below)? Each word in vocab gets tensor of size 125 to represent it.\n    Also 20 represents the vocabulary size. Meaning NO numerical value in the given tensor is allowed to exceed\n    20 or \"IndexError: index out of range in self\" error is thrown. '''\n\nnn.Embedding(20, 125)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:02:06.256486Z","iopub.execute_input":"2024-04-14T14:02:06.256798Z","iopub.status.idle":"2024-04-14T14:02:06.273204Z","shell.execute_reply.started":"2024-04-14T14:02:06.256772Z","shell.execute_reply":"2024-04-14T14:02:06.272364Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"Embedding(20, 125)"},"metadata":{}}]},{"cell_type":"code","source":"''' Demonstrating the \"IndexError: index out of range in self\" error. As stated before, embedding layer must take in numbers \n    that do not exceed the input size. Likely input size will match vocab size. Ex: If vocab is 5 words like dictionary \n    below, it won't be able to process a numerical value that's out of range of that dictionary. '''\n\n# t = testing\nt_dict = {\n    'hello':0,\n    'world':1,\n    'nice':2,\n    'seeing':3,\n    'you':4\n}\n\nt_vocab_size = len(t_dict)\nt_embedding_dim = 125\n\n# Input to embedding layer matches size of dictionary, which is 5 as of now. No value in tensor can match/exceed this value.\nel = nn.Embedding(t_vocab_size, t_embedding_dim)\nprint(f'Embedding layer dimensions in form (input, embedding_dim/output): {el}\\n')\n\n# A few tensors to try. Feel free to pass any one of them to the layer as done below.\nt_one = torch.tensor([4,3,1]) # Works\nt_two = torch.tensor([7,0]) # Does not work. 7 >= 5\nt_three = torch.tensor([5,1,1,1]) # Does not work. 5 >= 5\n\nt_four = torch.tensor([[1,2,4],\n                       [0,3,2]]) # Works\nt_five = torch.tensor([[3,4,1,10,2],\n                       [1,2,9,3, 0]]) # Does not work. 10 & 9 >= 5\n\n# Length is 3 because t_one has 3 values. An embedding vector of 125 (t_embedding_dim) is given for each one\noutput = el(t_one)\nprint(f'Output length: {len(output)}\\n\\nShape: {output.shape}\\n\\nOutput tensor:\\n{output}')","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:02:06.280880Z","iopub.execute_input":"2024-04-14T14:02:06.281355Z","iopub.status.idle":"2024-04-14T14:02:06.343978Z","shell.execute_reply.started":"2024-04-14T14:02:06.281320Z","shell.execute_reply":"2024-04-14T14:02:06.343151Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Embedding layer dimensions in form (input, embedding_dim/output): Embedding(5, 125)\n\nOutput length: 3\n\nShape: torch.Size([3, 125])\n\nOutput tensor:\ntensor([[ 5.6976e-01,  1.5177e-01, -7.6332e-01,  7.8860e-02,  1.4529e+00,\n          7.6839e-01, -3.5409e-01, -4.9864e-01,  6.0407e-01, -1.2675e+00,\n          8.9897e-02,  3.8728e-01, -3.6152e-01, -2.8312e+00, -2.1517e-02,\n          1.7260e+00,  2.3252e+00, -3.3742e-01, -5.0838e-01, -2.6107e-01,\n          1.2092e+00,  6.7096e-01, -4.7472e-01, -7.5652e-01, -1.9037e-02,\n         -5.8808e-01,  7.5883e-01, -7.6974e-01, -1.8932e+00,  1.2716e+00,\n          6.8654e-02, -1.6896e+00, -4.3401e-01, -4.5224e-01, -1.4260e+00,\n          9.0182e-01, -6.6335e-01, -2.6320e-01, -2.0138e+00,  1.1092e+00,\n          5.7241e-01,  6.4162e-01, -1.0330e+00,  2.7920e-02, -9.9227e-01,\n          9.5518e-01,  8.6002e-02, -1.4354e+00,  1.1145e-01, -1.2560e+00,\n         -7.3104e-01, -1.8002e+00,  1.1727e+00,  3.5310e-01,  4.4688e-01,\n         -2.8496e-01,  8.2814e-01, -7.2428e-01,  1.2443e-01, -2.6510e-01,\n          2.0939e+00,  8.8843e-02, -1.1740e+00,  1.5646e+00, -1.1530e+00,\n         -1.4210e+00, -3.0102e-01,  2.0164e-01,  3.4172e-01, -2.5101e-01,\n         -1.9481e+00,  7.2808e-01,  7.0708e-01,  2.3110e-01, -2.7645e-01,\n          7.4555e-01, -9.0218e-01, -9.4103e-01,  5.6481e-01,  3.1617e-01,\n          9.4517e-01, -1.5232e+00,  1.5047e-01,  6.5048e-02,  5.7977e-01,\n          5.9016e-01,  9.6858e-01,  4.5182e-01,  2.0158e+00,  6.6951e-04,\n         -2.6871e+00,  1.0194e+00,  9.7141e-01, -1.6673e+00,  1.3235e+00,\n         -1.5437e+00,  5.2571e-01,  5.9050e-01, -5.9614e-01,  7.6693e-01,\n         -3.2877e-03, -7.0755e-01, -6.3631e-01,  9.6273e-01, -3.6330e-01,\n          1.4867e+00,  6.1217e-01,  1.1277e-01,  5.8847e-01,  7.6961e-01,\n         -3.7273e-01, -1.9038e+00, -7.3505e-01, -1.9565e+00,  2.1898e-01,\n         -2.7586e-01,  3.3636e-01, -1.9381e-01, -7.9756e-01, -5.8014e-01,\n          3.9836e-01, -1.8805e+00, -1.3160e-01,  5.7258e-01,  1.3629e+00],\n        [ 1.3124e+00,  1.2059e+00,  6.3986e-01,  7.7748e-01,  7.9895e-01,\n         -3.8611e-01,  2.1242e+00, -6.6923e-01, -3.2123e-01,  4.3300e-01,\n         -9.9425e-01, -2.0950e+00,  2.1086e-01, -1.5352e+00, -2.1269e-01,\n         -1.4081e+00, -3.3692e-01,  1.5496e-01, -9.6232e-01,  1.2706e+00,\n         -3.7824e-02,  1.0595e+00,  1.8834e+00, -2.2425e-01, -3.4175e-01,\n          4.2434e-01,  1.9456e+00, -3.1257e-01, -1.4440e+00,  3.8318e-01,\n         -9.4740e-02, -6.4707e-01, -1.9272e+00, -9.4567e-01, -1.3116e-01,\n          1.2083e+00, -1.0399e+00, -2.1396e-01, -7.2102e-01,  5.5355e-01,\n          1.7444e-01, -1.4643e+00, -1.0089e-01, -7.4353e-01, -1.2142e+00,\n         -6.0408e-01, -7.1120e-01,  1.8397e+00, -1.0115e-01, -3.3492e-01,\n          4.1582e-01,  1.7838e+00, -1.2947e+00,  1.0691e+00, -5.8813e-01,\n         -9.7073e-02, -3.2939e-02, -6.0043e-01, -7.7724e-01,  7.1728e-01,\n         -1.9736e-01, -1.6886e+00,  2.2205e-01,  5.0235e-01, -2.2289e+00,\n          9.2651e-01,  5.4606e-02, -1.0829e-02, -1.0686e+00, -9.4922e-01,\n         -5.3428e-01, -8.8294e-01, -1.6101e-01,  1.4761e+00,  1.1887e+00,\n         -2.2464e+00,  5.8269e-01, -7.2536e-01, -8.4408e-01, -4.7109e-01,\n         -1.4293e+00, -1.3431e+00, -5.9224e-01, -1.2950e+00, -6.8333e-01,\n         -7.4047e-01,  5.6374e-01, -3.3023e-01,  1.7318e+00,  2.2392e+00,\n         -1.5771e-01, -1.6031e+00, -2.5427e+00, -1.2641e+00,  1.8396e-01,\n         -3.5977e-01, -3.1019e-01,  6.2400e-01, -1.8102e+00,  7.5810e-01,\n          5.9370e-01, -4.0246e-01, -8.6558e-01, -8.7390e-01, -5.2043e-01,\n         -2.0936e-01, -2.5481e-01, -1.3776e-01, -1.1539e+00,  1.5203e+00,\n          1.8910e+00,  7.6559e-01,  5.9840e-01,  4.0243e-01, -4.1903e-01,\n          9.4085e-01, -1.7400e-01, -8.6694e-01,  1.6498e+00, -4.9034e-01,\n         -3.9523e-01, -1.0858e+00, -1.0432e+00,  1.5125e+00, -6.9396e-01],\n        [ 7.3341e-01, -1.4738e+00,  4.7925e-01, -3.7310e-01,  4.4831e-01,\n         -3.1007e-01,  8.3873e-02, -6.7556e-01, -1.3219e+00,  1.1194e+00,\n          6.6266e-01,  2.3564e-01, -2.6754e-01, -3.2365e-01, -2.0899e+00,\n         -6.7511e-02, -8.7974e-01, -9.2885e-01, -9.7183e-02,  6.8311e-02,\n          1.4549e+00,  2.6665e-01, -6.5218e-01,  3.6197e-01,  6.5777e-01,\n         -5.3201e-01, -1.0584e-01,  1.6723e+00,  1.2359e+00,  8.1069e-01,\n         -4.7442e-01, -1.9242e+00, -3.6329e-01, -8.3990e-01,  1.7948e+00,\n         -9.7583e-02,  1.1261e+00, -5.3056e-01, -1.0480e+00, -1.0887e+00,\n          9.9493e-01,  8.2278e-02, -7.5653e-01,  1.8237e+00,  2.3717e-02,\n         -1.4223e+00,  9.1348e-01, -4.4574e-01, -5.1657e-01, -1.0323e+00,\n         -7.1830e-01,  5.4721e-01, -7.2834e-01,  9.2069e-01,  1.6755e+00,\n         -1.1279e+00,  1.1130e+00, -5.2613e-01,  4.1875e-01,  2.1825e+00,\n          8.2117e-02,  9.6759e-01,  2.1825e+00, -3.3092e-01, -7.8068e-01,\n          1.1858e-01,  1.1584e+00, -3.4937e-01,  1.9151e+00, -1.2056e+00,\n         -5.5242e-01, -4.0767e-01,  1.4140e+00,  1.3855e+00, -8.6180e-01,\n         -1.8523e+00,  6.4395e-01, -7.9744e-02, -3.5156e-01, -1.0918e+00,\n         -1.1955e-01,  7.2567e-01,  2.6259e-01,  4.2686e-02, -1.2276e+00,\n         -1.2897e-01, -1.5566e+00, -1.9107e-01,  6.0814e-01,  1.0655e+00,\n         -9.0842e-01, -8.2824e-01, -2.2479e+00,  6.5082e-01, -1.6769e-01,\n         -6.5670e-01, -6.5218e-01, -6.9328e-01,  2.2747e-01, -9.3050e-02,\n         -1.7114e+00,  6.2029e-01, -7.2064e-01, -1.1263e+00,  4.8771e-02,\n          1.5843e-02,  1.1858e+00, -1.9933e-01,  1.0809e-01, -1.7864e+00,\n          9.8139e-01, -3.5520e-01, -3.8285e-01,  1.2966e+00,  8.5518e-01,\n          7.1953e-01, -4.8083e-01, -1.9330e-01, -7.3782e-02,  8.4414e-01,\n          1.3812e+00,  1.6050e+00, -7.9621e-01,  1.3586e+00,  5.8799e-01]],\n       grad_fn=<EmbeddingBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"t_hidden_dim = 225\nt_layers = 1\n\n''' Output of embedding is input to lstm. Same concept for all layers.\n    Also, Lstm cells/layers return 2 values. \n    1) cell state (sometimes called \"out\") \n    2) hidden state.\n    Ex: If embedding dim is 100, that's the embedding out, so lstm will take that size\n        as input and things work because each numerical sentence is that size as well (see section 6-Padding\n        again for refresher). The hidden dimension is the size of both the cell state and hidden state. \n        Also since input_size to lstm is t_embedding_dim, that means input to lstm MUST be in shape \n        (1,1,t_embedding_dim) which is 3d. embedding dim is 100? Then (1,1,100)\n        \n        \n    3) layers - # of layers stacked. Ex: If given 2 and layers stacked, the output of 1 lstm is input\n        to the next. Can potentially increase accuracy. '''\n\nlstm_layer = nn.LSTM(input_size=t_embedding_dim, \n                     hidden_size=t_hidden_dim, \n                     num_layers=t_layers,\n                     batch_first=True)\n\nlstm_layer","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:02:06.345048Z","iopub.execute_input":"2024-04-14T14:02:06.345368Z","iopub.status.idle":"2024-04-14T14:02:06.358262Z","shell.execute_reply.started":"2024-04-14T14:02:06.345338Z","shell.execute_reply":"2024-04-14T14:02:06.357460Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"LSTM(125, 225, batch_first=True)"},"metadata":{}}]},{"cell_type":"code","source":"# Displays things about input to lstm layer, and returning cell & hidden states.\ndef layer_results(inp, layer, return_output=False):\n    print(f'Tensor input shape: {inp.shape}\\nTensor:\\n{inp}\\n\\n')\n    \n    output, hidden = layer(inp)\n    print(f'Output shape: {output.shape}\\nOutput tensor:\\n{output}\\n\\n')\n    print(f'Hidden state index 0 & index 1 shape (the same): {hidden[0].shape} - {hidden[1].shape}\\n')\n    print(f'Hidden state index 0 tensor:\\n{hidden[0]}\\n\\nHidden state index 1 tensor:\\n{hidden[1]}\\n\\n')\n    \n    if return_output is True:\n        return output, hidden","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:02:06.359613Z","iopub.execute_input":"2024-04-14T14:02:06.360221Z","iopub.status.idle":"2024-04-14T14:02:06.366225Z","shell.execute_reply.started":"2024-04-14T14:02:06.360191Z","shell.execute_reply":"2024-04-14T14:02:06.365327Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"''' Creating input data for layer in shape (batch_size, sequence_len, t_embedding_dim)\n    1) batch_size - Like batches of data in data loaders, how large will the data be?\n    2) seq_len - LSTM can take sequences of different lengths and output that at each \n        time step. If seq_len was 1, it gave 1 row in the input tensor. With 3? It\n        shows 3 rows of output data. '''\nbatch_size = 1\nseq_len = 1\n\ninput_tensor = torch.rand(batch_size, seq_len, t_embedding_dim)\n\n''' Test it on lstm. Remember:\n    1) Input is (batch_size, seq_len, t_embedding_dim) and lstm_layer is (t_embedding_dim, hidden_dim).\n        As long as the embedding_dim args work, everything is fine.\n        \n    2) lstm layers return cell state (sometimes called \"out\") & hidden state so the return value\n        of hidden will be a tuple.\n        \n        output - Shape is (batch_size, seq_len, hidden_dim)\n        hidden - BOTH shapes in tuple are same\n        \n'''\noutput, hidden = lstm_layer(input_tensor)\n\n# Both indices in tuple are shape\nlayer_results(input_tensor, lstm_layer)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:02:06.367449Z","iopub.execute_input":"2024-04-14T14:02:06.368053Z","iopub.status.idle":"2024-04-14T14:02:06.489434Z","shell.execute_reply.started":"2024-04-14T14:02:06.368023Z","shell.execute_reply":"2024-04-14T14:02:06.488418Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Tensor input shape: torch.Size([1, 1, 125])\nTensor:\ntensor([[[0.7383, 0.4637, 0.3988, 0.6387, 0.0808, 0.0762, 0.6612, 0.6489,\n          0.3081, 0.4039, 0.8276, 0.7073, 0.9309, 0.1596, 0.3816, 0.9431,\n          0.5988, 0.5196, 0.1280, 0.8004, 0.3358, 0.8920, 0.2427, 0.0168,\n          0.1399, 0.4779, 0.9491, 0.9021, 0.2783, 0.8918, 0.8733, 0.6173,\n          0.2620, 0.5533, 0.4058, 0.0399, 0.2062, 0.3062, 0.0954, 0.8915,\n          0.9668, 0.3066, 0.3716, 0.7563, 0.6928, 0.4147, 0.2433, 0.2421,\n          0.1672, 0.5924, 0.2476, 0.1676, 0.9590, 0.1739, 0.0732, 0.1268,\n          0.6689, 0.2571, 0.2972, 0.7000, 0.0771, 0.6547, 0.8618, 0.2774,\n          0.1293, 0.2652, 0.2995, 0.9032, 0.5428, 0.3935, 0.7635, 0.3996,\n          0.4589, 0.2435, 0.5511, 0.6553, 0.9424, 0.3509, 0.3234, 0.0216,\n          0.8781, 0.9482, 0.1711, 0.1117, 0.5140, 0.1179, 0.9131, 0.4670,\n          0.3850, 0.4878, 0.2537, 0.6619, 0.7156, 0.2417, 0.0158, 0.6523,\n          0.3362, 0.2348, 0.8367, 0.6179, 0.3198, 0.4962, 0.5002, 0.5078,\n          0.8378, 0.7198, 0.7172, 0.8032, 0.2536, 0.3239, 0.5959, 0.2257,\n          0.8649, 0.8055, 0.7282, 0.4163, 0.4378, 0.3733, 0.3841, 0.4383,\n          0.1275, 0.1624, 0.8384, 0.2114, 0.8247]]])\n\n\nOutput shape: torch.Size([1, 1, 225])\nOutput tensor:\ntensor([[[-3.8164e-02, -1.0971e-01,  4.2686e-02,  7.8706e-02, -3.3887e-02,\n          -7.8257e-02, -3.4943e-02, -7.3273e-02,  1.9917e-02, -2.4210e-02,\n          -2.5264e-02, -1.2930e-02,  2.5134e-02,  3.2181e-03, -8.0142e-02,\n          -4.6176e-02,  2.4570e-02,  7.1582e-03,  6.2854e-02,  3.0538e-02,\n           3.4635e-02,  5.3741e-02,  1.3396e-02, -1.7749e-02,  2.6181e-02,\n          -2.3136e-02,  6.6826e-02, -1.8155e-02,  8.8380e-02,  3.8965e-02,\n           5.9279e-02,  6.4411e-02,  1.1188e-02, -4.9834e-02,  4.2440e-02,\n          -6.9359e-02,  2.2296e-02, -4.5073e-02, -3.9172e-02,  2.6072e-02,\n          -2.9962e-03, -2.0013e-02,  1.3014e-02, -9.9442e-04,  8.1946e-02,\n          -1.2925e-02, -8.9233e-02,  1.3493e-03,  4.4232e-02,  2.9549e-02,\n          -5.7906e-03,  3.6220e-02, -6.6202e-02, -1.0188e-01,  7.5185e-02,\n           2.5421e-02, -5.4765e-03, -4.4423e-02,  1.9236e-02, -2.7279e-02,\n           4.3252e-02,  3.6515e-02, -3.4231e-02,  3.1829e-02,  3.7431e-02,\n          -1.4876e-03, -3.7946e-02,  8.8430e-02, -4.1950e-02, -3.7119e-02,\n           5.3857e-02, -1.9754e-02,  7.3036e-02,  8.8741e-03,  1.5443e-02,\n           5.1469e-02,  1.5634e-02,  1.6525e-02, -1.0302e-01, -6.7243e-03,\n          -1.8740e-02,  4.2932e-02,  3.0525e-02,  8.9293e-02, -2.8285e-02,\n           3.0085e-02,  8.4854e-02,  2.5968e-03,  5.4215e-02,  7.4466e-02,\n           5.4583e-02,  7.8999e-02, -8.4970e-02, -8.1807e-02, -8.1283e-02,\n          -2.6924e-02, -1.0156e-01,  1.4707e-02, -9.3111e-03,  1.5929e-02,\n          -6.1010e-02,  6.6488e-02,  4.9415e-02, -7.3265e-02,  3.5866e-02,\n           9.8751e-02, -9.4374e-03, -1.6271e-02, -7.9978e-02, -9.0439e-02,\n           6.6686e-02, -4.0533e-02,  5.1700e-03,  1.4874e-01, -3.6221e-02,\n          -8.7340e-02,  2.5463e-02, -5.6019e-02,  1.2654e-02,  5.2893e-02,\n          -1.5286e-02, -8.4789e-02, -2.0496e-02,  7.2416e-02, -3.5393e-02,\n          -1.0876e-01, -1.7945e-02,  1.3144e-01,  9.7856e-02, -6.2689e-02,\n          -8.0168e-02,  7.9030e-02,  1.4250e-02, -7.3242e-03, -4.4137e-05,\n           7.5629e-02, -8.1998e-03, -4.4594e-02, -3.1254e-02,  8.1285e-02,\n           5.5514e-02, -3.1852e-02, -2.0224e-02,  1.5670e-03,  2.2719e-02,\n          -9.8545e-02,  9.3447e-03,  1.1149e-02, -5.6420e-03, -3.0279e-02,\n           5.0330e-02,  5.2607e-02, -8.7429e-02,  6.1963e-04, -6.3881e-02,\n           2.1323e-02,  4.7929e-02, -1.2571e-01, -2.0849e-02, -3.6644e-02,\n           9.1232e-02,  3.2530e-02, -2.4916e-02,  5.7005e-02,  1.1270e-01,\n           1.2552e-01, -2.8125e-02, -2.3208e-02,  6.7054e-02, -3.8026e-02,\n           3.4553e-02, -3.2471e-02,  6.7828e-02,  4.6149e-02, -5.8097e-03,\n           1.5555e-03, -1.6057e-01, -4.2139e-03, -8.6765e-02,  9.5795e-02,\n          -2.8265e-03, -9.2249e-03, -6.1457e-02, -5.8397e-02, -6.8844e-02,\n          -9.9909e-02, -1.3103e-01, -2.0295e-02, -4.9821e-02, -5.6771e-02,\n           6.0542e-03, -8.9904e-02,  1.8332e-02, -9.1034e-02, -3.4155e-03,\n          -2.6255e-02, -1.2304e-02,  8.8123e-02,  1.9138e-02, -5.1648e-02,\n          -1.3408e-01, -4.1662e-02,  2.1983e-02, -6.8019e-02, -4.9661e-03,\n           1.8390e-02,  1.7916e-02,  1.1237e-01, -3.9237e-02,  1.4636e-02,\n           3.3732e-02, -2.4535e-02,  8.2260e-02,  7.6479e-03, -5.8327e-02,\n          -7.1157e-02,  1.2517e-02,  3.4046e-02, -4.4600e-02, -4.8846e-02,\n           4.9692e-02,  7.4984e-02,  2.6717e-02,  2.7915e-02,  1.5443e-02]]],\n       grad_fn=<TransposeBackward0>)\n\n\nHidden state index 0 & index 1 shape (the same): torch.Size([1, 1, 225]) - torch.Size([1, 1, 225])\n\nHidden state index 0 tensor:\ntensor([[[-3.8164e-02, -1.0971e-01,  4.2686e-02,  7.8706e-02, -3.3887e-02,\n          -7.8257e-02, -3.4943e-02, -7.3273e-02,  1.9917e-02, -2.4210e-02,\n          -2.5264e-02, -1.2930e-02,  2.5134e-02,  3.2181e-03, -8.0142e-02,\n          -4.6176e-02,  2.4570e-02,  7.1582e-03,  6.2854e-02,  3.0538e-02,\n           3.4635e-02,  5.3741e-02,  1.3396e-02, -1.7749e-02,  2.6181e-02,\n          -2.3136e-02,  6.6826e-02, -1.8155e-02,  8.8380e-02,  3.8965e-02,\n           5.9279e-02,  6.4411e-02,  1.1188e-02, -4.9834e-02,  4.2440e-02,\n          -6.9359e-02,  2.2296e-02, -4.5073e-02, -3.9172e-02,  2.6072e-02,\n          -2.9962e-03, -2.0013e-02,  1.3014e-02, -9.9442e-04,  8.1946e-02,\n          -1.2925e-02, -8.9233e-02,  1.3493e-03,  4.4232e-02,  2.9549e-02,\n          -5.7906e-03,  3.6220e-02, -6.6202e-02, -1.0188e-01,  7.5185e-02,\n           2.5421e-02, -5.4765e-03, -4.4423e-02,  1.9236e-02, -2.7279e-02,\n           4.3252e-02,  3.6515e-02, -3.4231e-02,  3.1829e-02,  3.7431e-02,\n          -1.4876e-03, -3.7946e-02,  8.8430e-02, -4.1950e-02, -3.7119e-02,\n           5.3857e-02, -1.9754e-02,  7.3036e-02,  8.8741e-03,  1.5443e-02,\n           5.1469e-02,  1.5634e-02,  1.6525e-02, -1.0302e-01, -6.7243e-03,\n          -1.8740e-02,  4.2932e-02,  3.0525e-02,  8.9293e-02, -2.8285e-02,\n           3.0085e-02,  8.4854e-02,  2.5968e-03,  5.4215e-02,  7.4466e-02,\n           5.4583e-02,  7.8999e-02, -8.4970e-02, -8.1807e-02, -8.1283e-02,\n          -2.6924e-02, -1.0156e-01,  1.4707e-02, -9.3111e-03,  1.5929e-02,\n          -6.1010e-02,  6.6488e-02,  4.9415e-02, -7.3265e-02,  3.5866e-02,\n           9.8751e-02, -9.4374e-03, -1.6271e-02, -7.9978e-02, -9.0439e-02,\n           6.6686e-02, -4.0533e-02,  5.1700e-03,  1.4874e-01, -3.6221e-02,\n          -8.7340e-02,  2.5463e-02, -5.6019e-02,  1.2654e-02,  5.2893e-02,\n          -1.5286e-02, -8.4789e-02, -2.0496e-02,  7.2416e-02, -3.5393e-02,\n          -1.0876e-01, -1.7945e-02,  1.3144e-01,  9.7856e-02, -6.2689e-02,\n          -8.0168e-02,  7.9030e-02,  1.4250e-02, -7.3242e-03, -4.4137e-05,\n           7.5629e-02, -8.1998e-03, -4.4594e-02, -3.1254e-02,  8.1285e-02,\n           5.5514e-02, -3.1852e-02, -2.0224e-02,  1.5670e-03,  2.2719e-02,\n          -9.8545e-02,  9.3447e-03,  1.1149e-02, -5.6420e-03, -3.0279e-02,\n           5.0330e-02,  5.2607e-02, -8.7429e-02,  6.1963e-04, -6.3881e-02,\n           2.1323e-02,  4.7929e-02, -1.2571e-01, -2.0849e-02, -3.6644e-02,\n           9.1232e-02,  3.2530e-02, -2.4916e-02,  5.7005e-02,  1.1270e-01,\n           1.2552e-01, -2.8125e-02, -2.3208e-02,  6.7054e-02, -3.8026e-02,\n           3.4553e-02, -3.2471e-02,  6.7828e-02,  4.6149e-02, -5.8097e-03,\n           1.5555e-03, -1.6057e-01, -4.2139e-03, -8.6765e-02,  9.5795e-02,\n          -2.8265e-03, -9.2249e-03, -6.1457e-02, -5.8397e-02, -6.8844e-02,\n          -9.9909e-02, -1.3103e-01, -2.0295e-02, -4.9821e-02, -5.6771e-02,\n           6.0542e-03, -8.9904e-02,  1.8332e-02, -9.1034e-02, -3.4155e-03,\n          -2.6255e-02, -1.2304e-02,  8.8123e-02,  1.9138e-02, -5.1648e-02,\n          -1.3408e-01, -4.1662e-02,  2.1983e-02, -6.8019e-02, -4.9661e-03,\n           1.8390e-02,  1.7916e-02,  1.1237e-01, -3.9237e-02,  1.4636e-02,\n           3.3732e-02, -2.4535e-02,  8.2260e-02,  7.6479e-03, -5.8327e-02,\n          -7.1157e-02,  1.2517e-02,  3.4046e-02, -4.4600e-02, -4.8846e-02,\n           4.9692e-02,  7.4984e-02,  2.6717e-02,  2.7915e-02,  1.5443e-02]]],\n       grad_fn=<StackBackward0>)\n\nHidden state index 1 tensor:\ntensor([[[-9.6017e-02, -1.8944e-01,  8.6623e-02,  1.4164e-01, -7.6791e-02,\n          -1.4175e-01, -7.5936e-02, -1.6698e-01,  3.5707e-02, -4.8857e-02,\n          -5.7869e-02, -2.9756e-02,  4.8077e-02,  7.0668e-03, -1.4252e-01,\n          -8.1178e-02,  4.4556e-02,  1.3929e-02,  1.2341e-01,  5.4902e-02,\n           6.1228e-02,  9.7512e-02,  3.1504e-02, -3.4377e-02,  5.6839e-02,\n          -4.9098e-02,  1.6432e-01, -4.0005e-02,  1.7016e-01,  8.5239e-02,\n           1.2216e-01,  1.4009e-01,  2.3200e-02, -1.0595e-01,  9.0014e-02,\n          -1.4668e-01,  4.4322e-02, -8.5114e-02, -7.8384e-02,  5.6106e-02,\n          -5.8136e-03, -4.2546e-02,  2.2842e-02, -1.9912e-03,  1.5991e-01,\n          -2.4359e-02, -1.5520e-01,  2.7132e-03,  9.5638e-02,  7.0647e-02,\n          -1.1357e-02,  8.7043e-02, -1.2671e-01, -1.8469e-01,  1.6856e-01,\n           4.9757e-02, -1.0230e-02, -1.0449e-01,  3.1419e-02, -5.3117e-02,\n           8.0053e-02,  6.7553e-02, -7.2160e-02,  5.2744e-02,  6.1273e-02,\n          -2.7441e-03, -7.4598e-02,  1.6118e-01, -7.8042e-02, -8.4648e-02,\n           1.0336e-01, -3.3363e-02,  1.6787e-01,  1.7274e-02,  3.4412e-02,\n           1.2163e-01,  3.6762e-02,  2.8470e-02, -1.8688e-01, -1.1252e-02,\n          -3.6829e-02,  8.5221e-02,  6.9364e-02,  1.8805e-01, -5.7749e-02,\n           6.8682e-02,  1.8033e-01,  5.3363e-03,  1.1125e-01,  1.6190e-01,\n           1.2175e-01,  1.5749e-01, -1.9362e-01, -1.6509e-01, -1.3384e-01,\n          -5.8199e-02, -2.3662e-01,  2.6024e-02, -1.8834e-02,  2.9640e-02,\n          -1.2214e-01,  1.4631e-01,  9.5757e-02, -1.6132e-01,  7.1169e-02,\n           1.8267e-01, -2.2510e-02, -3.4881e-02, -1.6475e-01, -1.7451e-01,\n           1.3017e-01, -7.4010e-02,  1.2921e-02,  2.6881e-01, -8.5389e-02,\n          -1.8376e-01,  6.3865e-02, -1.1744e-01,  2.7423e-02,  1.0809e-01,\n          -2.9642e-02, -1.4602e-01, -3.8490e-02,  1.3022e-01, -6.0911e-02,\n          -1.9917e-01, -3.6093e-02,  2.2886e-01,  1.8090e-01, -1.4810e-01,\n          -1.5606e-01,  1.5215e-01,  2.9230e-02, -1.4049e-02, -8.4234e-05,\n           1.4849e-01, -1.7866e-02, -8.2114e-02, -5.3290e-02,  1.5050e-01,\n           1.2940e-01, -7.8131e-02, -4.7131e-02,  3.0263e-03,  3.6286e-02,\n          -1.7816e-01,  1.9164e-02,  2.1873e-02, -1.1064e-02, -7.6406e-02,\n           8.7618e-02,  1.2060e-01, -1.6762e-01,  1.4170e-03, -1.3204e-01,\n           4.9694e-02,  1.0880e-01, -2.8615e-01, -5.3490e-02, -8.6161e-02,\n           2.1576e-01,  6.3336e-02, -4.6381e-02,  1.3024e-01,  2.2887e-01,\n           2.4422e-01, -6.9617e-02, -5.9358e-02,  1.3437e-01, -9.3699e-02,\n           7.5163e-02, -6.0879e-02,  1.4582e-01,  1.0046e-01, -1.2306e-02,\n           3.7946e-03, -2.9567e-01, -8.0213e-03, -1.9594e-01,  2.0105e-01,\n          -5.6561e-03, -2.0786e-02, -1.1633e-01, -1.3403e-01, -1.3208e-01,\n          -2.0455e-01, -2.3847e-01, -3.6729e-02, -9.3524e-02, -1.3631e-01,\n           1.2136e-02, -1.6777e-01,  3.5938e-02, -1.7873e-01, -5.7656e-03,\n          -5.1887e-02, -2.4068e-02,  1.9389e-01,  3.8717e-02, -1.0236e-01,\n          -2.7540e-01, -7.6749e-02,  4.7765e-02, -1.1963e-01, -1.1726e-02,\n           3.9462e-02,  3.7147e-02,  2.4400e-01, -7.9247e-02,  2.8377e-02,\n           7.4928e-02, -4.9765e-02,  1.8489e-01,  1.5198e-02, -9.3812e-02,\n          -1.5599e-01,  2.2253e-02,  7.1732e-02, -8.4500e-02, -1.0487e-01,\n           9.3953e-02,  1.5996e-01,  5.8277e-02,  5.5283e-02,  2.6864e-02]]],\n       grad_fn=<StackBackward0>)\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"''' Input here is 1,4,125. Below now displays input tensor being 3d but still has 4 rows.\n    of data to handle. '''\nx = torch.rand(batch_size, 4, t_embedding_dim)\n\nlayer_results(x, lstm_layer)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:02:06.490930Z","iopub.execute_input":"2024-04-14T14:02:06.491208Z","iopub.status.idle":"2024-04-14T14:02:06.570055Z","shell.execute_reply.started":"2024-04-14T14:02:06.491186Z","shell.execute_reply":"2024-04-14T14:02:06.569027Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Tensor input shape: torch.Size([1, 4, 125])\nTensor:\ntensor([[[0.3275, 0.7338, 0.5848, 0.4072, 0.6632, 0.7099, 0.5751, 0.0542,\n          0.7990, 0.4986, 0.3432, 0.9094, 0.1177, 0.0467, 0.1572, 0.0657,\n          0.3597, 0.7513, 0.3565, 0.1548, 0.9303, 0.6180, 0.1215, 0.2969,\n          0.0977, 0.6954, 0.3726, 0.0495, 0.9438, 0.1519, 0.7127, 0.0586,\n          0.2709, 0.6485, 0.6501, 0.8061, 0.2822, 0.5306, 0.3691, 0.9911,\n          0.1978, 0.3716, 0.3575, 0.5769, 0.2587, 0.9969, 0.9139, 0.3377,\n          0.7628, 0.5335, 0.9762, 0.3061, 0.4698, 0.5040, 0.3976, 0.4218,\n          0.4961, 0.1177, 0.3488, 0.8712, 0.9263, 0.9677, 0.9990, 0.0091,\n          0.2032, 0.9661, 0.7577, 0.9482, 0.8548, 0.3366, 0.5036, 0.4455,\n          0.9580, 0.0211, 0.2232, 0.1863, 0.4000, 0.1535, 0.9884, 0.1924,\n          0.2789, 0.4663, 0.0281, 0.4863, 0.7781, 0.2190, 0.5725, 0.2361,\n          0.8896, 0.1857, 0.8446, 0.6156, 0.1285, 0.4113, 0.6130, 0.3031,\n          0.1703, 0.1622, 0.6914, 0.3480, 0.3343, 0.2213, 0.0271, 0.4933,\n          0.2153, 0.9069, 0.1814, 0.6010, 0.3441, 0.8863, 0.4193, 0.9191,\n          0.7838, 0.4861, 0.0956, 0.5272, 0.6293, 0.1491, 0.2740, 0.0634,\n          0.6689, 0.6835, 0.2956, 0.8191, 0.2930],\n         [0.8136, 0.1423, 0.9651, 0.3957, 0.4896, 0.4095, 0.4698, 0.3213,\n          0.2265, 0.7458, 0.0160, 0.6114, 0.6657, 0.2183, 0.6003, 0.6167,\n          0.7907, 0.2667, 0.0406, 0.0370, 0.4718, 0.8596, 0.2610, 0.6216,\n          0.3000, 0.1528, 0.0497, 0.2227, 0.5905, 0.8919, 0.3462, 0.9686,\n          0.8548, 0.8701, 0.6844, 0.2227, 0.9295, 0.2151, 0.7208, 0.0266,\n          0.7892, 0.1147, 0.2042, 0.9613, 0.1551, 0.0101, 0.4908, 0.0688,\n          0.4169, 0.0728, 0.8076, 0.7330, 0.2432, 0.6823, 0.7193, 0.0042,\n          0.3654, 0.3779, 0.4464, 0.0387, 0.7060, 0.3086, 0.1252, 0.9854,\n          0.1759, 0.0299, 0.3506, 0.5099, 0.1441, 0.2083, 0.8835, 0.8863,\n          0.5541, 0.4839, 0.2039, 0.7131, 0.3712, 0.7818, 0.6527, 0.1937,\n          0.2436, 0.6444, 0.7291, 0.4041, 0.9054, 0.4338, 0.1994, 0.7060,\n          0.6127, 0.6183, 0.4718, 0.5678, 0.4879, 0.2899, 0.2144, 0.4630,\n          0.9295, 0.7427, 0.8633, 0.1565, 0.5975, 0.4071, 0.3344, 0.3226,\n          0.5812, 0.3283, 0.4624, 0.1750, 0.0291, 0.9197, 0.7576, 0.2564,\n          0.7176, 0.7473, 0.2023, 0.2102, 0.0976, 0.9838, 0.1316, 0.1704,\n          0.4405, 0.9250, 0.9022, 0.8227, 0.6131],\n         [0.3140, 0.6185, 0.6268, 0.0129, 0.5326, 0.6032, 0.4958, 0.6723,\n          0.6806, 0.4628, 0.0883, 0.9561, 0.6920, 0.5679, 0.6504, 0.9399,\n          0.0328, 0.2837, 0.8342, 0.5159, 0.3291, 0.8521, 0.0583, 0.6528,\n          0.0292, 0.9998, 0.4828, 0.2150, 0.0177, 0.9174, 0.6107, 0.5466,\n          0.8901, 0.5132, 0.3670, 0.7641, 0.0347, 0.1921, 0.4200, 0.5968,\n          0.9064, 0.3058, 0.2007, 0.5875, 0.3382, 0.5213, 0.3867, 0.8248,\n          0.1578, 0.2894, 0.4309, 0.5444, 0.3906, 0.5971, 0.8961, 0.8651,\n          0.6267, 0.8533, 0.1236, 0.9194, 0.4225, 0.7046, 0.7114, 0.3467,\n          0.2611, 0.9427, 0.0188, 0.0536, 0.5802, 0.7820, 0.1067, 0.2729,\n          0.9955, 0.9737, 0.0378, 0.8367, 0.7501, 0.3675, 0.1531, 0.6389,\n          0.1448, 0.9015, 0.4580, 0.3902, 0.3871, 0.3107, 0.6196, 0.9816,\n          0.7461, 0.9686, 0.2193, 0.6642, 0.3786, 0.4441, 0.3603, 0.1307,\n          0.7425, 0.0511, 0.1336, 0.5655, 0.9479, 0.0412, 0.7998, 0.1773,\n          0.0236, 0.8671, 0.2195, 0.1429, 0.4354, 0.9409, 0.4015, 0.1729,\n          0.7415, 0.9400, 0.1968, 0.3024, 0.2262, 0.7634, 0.9239, 0.2377,\n          0.3722, 0.4762, 0.0757, 0.4140, 0.5356],\n         [0.9366, 0.4397, 0.0552, 0.9518, 0.6228, 0.5770, 0.8434, 0.5327,\n          0.4946, 0.1935, 0.0191, 0.3304, 0.8430, 0.5487, 0.0604, 0.1760,\n          0.4908, 0.0207, 0.6909, 0.0237, 0.6631, 0.3833, 0.3485, 0.6180,\n          0.3671, 0.9916, 0.7768, 0.2295, 0.6602, 0.9193, 0.2567, 0.6868,\n          0.6726, 0.3070, 0.3373, 0.6628, 0.8517, 0.5051, 0.0334, 0.5824,\n          0.9323, 0.1465, 0.9621, 0.3860, 0.0776, 0.8000, 0.1374, 0.3905,\n          0.2157, 0.6191, 0.3180, 0.0839, 0.8179, 0.3808, 0.5207, 0.8792,\n          0.4801, 0.3790, 0.6964, 0.9311, 0.1051, 0.5590, 0.6143, 0.6109,\n          0.4829, 0.1821, 0.6341, 0.1963, 0.1977, 0.1296, 0.0797, 0.8774,\n          0.2453, 0.3155, 0.7303, 0.4155, 0.7152, 0.1988, 0.8255, 0.9167,\n          0.9733, 0.2520, 0.4084, 0.6801, 0.4030, 0.7315, 0.5104, 0.9421,\n          0.8880, 0.0756, 0.5726, 0.2490, 0.0862, 0.3604, 0.7096, 0.6214,\n          0.9947, 0.3846, 0.3305, 0.5848, 0.4925, 0.5216, 0.4282, 0.2009,\n          0.4583, 0.4523, 0.1275, 0.1724, 0.2248, 0.8268, 0.1539, 0.2741,\n          0.9422, 0.1256, 0.7719, 0.0981, 0.6821, 0.1999, 0.9387, 0.4462,\n          0.8275, 0.5992, 0.6652, 0.1984, 0.1693]]])\n\n\nOutput shape: torch.Size([1, 4, 225])\nOutput tensor:\ntensor([[[ 9.5845e-03, -5.0222e-02, -3.5763e-02, -3.9140e-02, -3.8474e-02,\n          -7.9342e-02, -2.7101e-02, -1.8754e-02, -4.3442e-03, -9.4145e-03,\n           2.4370e-03, -1.1153e-02, -8.9182e-02, -3.0565e-02, -1.1645e-01,\n           3.2562e-03,  1.4120e-02, -5.0022e-02, -7.9678e-04, -1.5047e-02,\n           9.1512e-02,  1.6878e-02, -9.9217e-03,  1.6464e-02, -2.8700e-02,\n          -1.4660e-02,  4.4721e-02, -7.6556e-02,  5.4101e-02, -2.8455e-02,\n           1.0264e-01,  6.5406e-02, -4.3853e-02, -1.4300e-04,  7.2364e-02,\n          -1.5991e-02,  6.2469e-02, -6.3239e-02, -4.6849e-02,  3.9509e-02,\n           2.5655e-03, -1.5929e-02,  3.6998e-02,  2.1926e-02,  5.7217e-02,\n           2.7013e-02, -1.5741e-01,  1.2635e-02,  3.6390e-02,  5.2415e-02,\n          -1.8301e-02,  2.8806e-02, -5.5603e-03, -1.8494e-01,  1.3281e-01,\n          -5.0491e-02,  4.8259e-02,  4.9443e-03,  2.2911e-02, -5.4499e-02,\n           1.0030e-01,  7.7357e-02,  5.3604e-02,  4.2562e-02,  8.5173e-02,\n          -1.1799e-02, -3.8474e-02,  8.7787e-02, -6.3551e-02, -2.0951e-02,\n          -4.7323e-02, -5.3348e-02,  5.3690e-02,  5.1293e-02, -2.6005e-02,\n           1.0000e-01, -2.9753e-03, -3.7680e-02, -1.0425e-01,  4.2674e-02,\n          -1.2518e-02,  4.2799e-02,  5.6940e-02,  2.7485e-02, -1.9336e-02,\n           1.8805e-02,  5.4446e-02, -1.7803e-02,  7.9118e-02,  4.1241e-02,\n           4.9336e-02,  1.1183e-01,  5.1233e-02, -1.0251e-01, -5.4691e-02,\n          -6.2014e-02, -5.4576e-02,  5.6402e-02, -6.1295e-02,  3.3686e-02,\n          -1.2639e-01, -3.3382e-02,  4.9822e-02, -5.9481e-02,  4.4984e-02,\n           7.9699e-03, -5.9320e-02, -1.0887e-01, -1.1647e-01, -4.5660e-02,\n           5.7964e-02, -9.5591e-03, -2.1752e-03,  9.3061e-02, -4.5660e-02,\n          -7.8269e-02,  2.4980e-02, -3.6108e-02,  2.8661e-04, -1.8318e-02,\n           8.5589e-03, -2.1360e-02, -6.8464e-02,  1.5871e-02,  2.6990e-02,\n          -1.4403e-01,  4.9040e-02,  9.9257e-02,  8.2066e-02,  4.2578e-03,\n           1.3077e-02,  8.3163e-02, -1.8974e-02,  7.6528e-02,  4.4391e-02,\n           3.5800e-02, -5.1500e-02, -4.6550e-02, -1.0405e-01,  8.2973e-02,\n          -4.4843e-03, -3.8561e-02, -8.8449e-03,  6.9474e-03,  4.1266e-02,\n          -1.2327e-01,  8.3345e-03, -2.7406e-02, -3.5253e-02, -2.1602e-02,\n           6.9977e-02,  3.2199e-02, -1.0426e-01,  2.3645e-02, -3.0137e-02,\n          -9.6592e-03,  7.0583e-02, -1.3430e-01,  2.4162e-02, -2.0071e-02,\n           1.1590e-01, -3.7768e-03,  8.9587e-04,  1.2643e-02,  9.9847e-02,\n           1.0295e-01,  1.3603e-02, -5.7012e-02,  6.5417e-02, -1.7044e-02,\n           1.2699e-02, -9.6921e-02,  5.2390e-02,  7.8067e-02, -6.0974e-02,\n           3.7641e-02, -1.1911e-01,  3.5314e-03, -7.4935e-02,  3.6211e-02,\n          -5.2796e-02,  8.4030e-03,  1.0925e-02, -5.1827e-02,  4.0474e-04,\n          -8.5434e-02, -6.0663e-02, -6.4710e-02, -8.0635e-02, -3.7925e-02,\n          -3.6555e-02, -6.8644e-02,  2.8899e-03, -7.9731e-02, -1.5917e-02,\n          -3.5105e-02,  3.5828e-02,  2.7179e-02,  1.6724e-03, -1.0232e-01,\n          -1.2901e-01, -2.5357e-02, -1.6690e-02, -2.8120e-03,  8.8895e-03,\n           1.4655e-03, -1.4523e-02,  9.1426e-02, -2.7858e-04,  4.3858e-02,\n          -1.2203e-02,  1.3508e-02,  9.3386e-02, -3.3840e-02, -2.2514e-02,\n          -5.8711e-02,  5.2939e-02,  9.6846e-02, -6.3215e-02, -5.8304e-02,\n          -4.3770e-02,  8.1901e-02,  6.5567e-02,  6.8423e-02, -9.2360e-03],\n         [-5.4250e-03, -1.0066e-01, -1.4631e-02, -4.7021e-03, -6.0921e-02,\n          -1.5351e-01,  2.1973e-02, -4.0332e-02, -2.4776e-02,  3.2526e-02,\n          -5.6134e-02, -7.5379e-02, -1.0276e-01, -2.0070e-02, -1.0237e-01,\n           2.3576e-02,  4.2427e-02, -7.6459e-02, -1.2382e-02, -4.1628e-02,\n           1.2565e-01, -1.3181e-02, -9.0798e-02, -6.3583e-03,  5.0105e-02,\n          -2.6319e-02,  8.2191e-02, -1.3042e-01,  1.0786e-01,  3.0661e-02,\n           1.0535e-01,  1.0307e-01, -8.8799e-02, -5.4221e-02,  4.5754e-02,\n          -2.1467e-02,  9.3350e-02, -6.8384e-02, -5.2257e-02, -1.5926e-02,\n           1.3543e-02, -4.2369e-02,  6.9631e-02,  7.4057e-02,  1.4387e-01,\n          -9.1668e-04, -2.2660e-01,  1.5055e-02,  4.2027e-02,  6.0970e-02,\n           4.1547e-02,  1.0978e-02, -8.0010e-02, -2.6766e-01,  1.7088e-01,\n          -1.0157e-01,  5.1435e-02, -2.1646e-02,  1.7710e-02, -5.7121e-02,\n           9.1720e-02,  8.2495e-02,  1.2152e-02, -5.9113e-03,  9.0989e-02,\n          -4.5772e-02, -6.0951e-02,  9.2451e-02, -1.2067e-01, -1.6255e-02,\n          -1.5504e-02, -8.6819e-02,  8.9192e-02,  2.2587e-02, -4.7228e-02,\n           1.1132e-01, -9.9508e-03, -7.8961e-02, -1.3629e-01,  7.1570e-02,\n          -7.9076e-04,  6.5721e-02,  3.2185e-02,  4.4321e-02, -2.0802e-02,\n           2.9728e-02,  4.0396e-02,  2.9700e-02,  8.8604e-02,  4.5261e-02,\n           1.0877e-01,  1.3351e-01,  1.2997e-02, -1.6611e-01, -9.1867e-02,\n          -4.6873e-02, -1.1759e-01, -1.1737e-02, -3.4565e-02,  4.4642e-02,\n          -1.4485e-01,  6.9288e-02,  4.6934e-02, -1.1877e-01,  1.0835e-01,\n           1.3681e-03, -6.5257e-02, -8.8815e-02, -1.2674e-01, -8.6446e-02,\n          -1.7273e-02, -4.1459e-02, -5.1836e-02,  8.3631e-02, -7.4531e-02,\n          -8.9265e-02,  3.9724e-02, -3.5709e-02,  2.1261e-02,  3.4546e-02,\n          -7.3965e-03, -6.6538e-02, -8.2364e-02, -1.8831e-02,  4.8157e-02,\n          -1.4188e-01,  4.0404e-02,  1.6179e-01,  7.7826e-02, -4.1267e-02,\n          -6.6809e-02,  8.4959e-02, -1.1731e-02, -1.6732e-02,  1.9505e-02,\n           7.9838e-02, -5.2688e-02, -4.5801e-02, -1.0585e-01,  8.5647e-02,\n           1.0405e-02, -4.6409e-02, -5.0387e-02, -5.3030e-02,  5.1499e-02,\n          -1.6732e-01,  1.8328e-02,  2.6944e-02, -1.1326e-02,  1.4644e-03,\n           7.2548e-02,  6.6213e-02, -1.4255e-01,  3.4528e-02, -4.3802e-03,\n           3.2878e-02,  1.7132e-01, -1.2475e-01,  4.2862e-02, -3.0828e-02,\n           1.4823e-01, -1.7488e-02,  6.2442e-02, -3.8760e-02,  1.7812e-01,\n           1.7185e-01, -8.1886e-03, -1.0629e-01,  8.9213e-02, -3.9530e-04,\n           9.9396e-02, -1.5622e-01,  9.8331e-02,  6.1966e-02, -5.7785e-02,\n           5.7836e-02, -1.3212e-01,  2.6496e-02, -1.1698e-02,  6.8098e-02,\n          -3.4611e-02, -3.1379e-02,  7.6891e-02, -1.3599e-01, -4.8864e-02,\n          -1.6612e-01, -9.3178e-02, -7.1662e-02, -4.4834e-02, -1.0210e-01,\n          -1.9890e-02, -8.7791e-02,  1.8078e-02, -1.1857e-01,  3.7192e-02,\n           4.1896e-02,  7.1992e-02,  3.3290e-02,  3.6193e-02, -1.2734e-01,\n          -2.0523e-01,  1.6741e-02,  6.1548e-02, -1.2699e-01,  5.2334e-03,\n           1.7748e-02,  2.3221e-03,  1.2436e-01,  4.7028e-02, -3.2517e-02,\n           3.8055e-03,  2.2338e-03,  1.3546e-01, -4.0683e-02, -8.4048e-02,\n          -6.4190e-02,  1.3179e-01,  1.1553e-01, -9.1905e-02, -9.2995e-02,\n           5.9292e-03,  3.7353e-02,  8.9358e-02,  3.0703e-02,  3.3732e-02],\n         [-2.8465e-02, -1.4181e-01,  2.9807e-04,  2.3933e-02, -8.4401e-02,\n          -2.0860e-01,  3.7972e-02, -8.9246e-02,  4.0158e-02,  2.9178e-02,\n          -3.5674e-02, -5.6564e-02, -4.8105e-02, -1.3026e-02, -1.2196e-01,\n          -8.9502e-03,  9.4748e-02, -4.9540e-03,  5.7695e-02,  6.8057e-02,\n           1.0826e-01, -2.8398e-02, -5.3736e-02, -1.2859e-02,  7.3588e-02,\n           2.6621e-02,  1.2303e-01, -1.4053e-01,  1.2813e-01, -1.2285e-03,\n           1.1538e-01,  8.4734e-02,  9.1856e-03, -8.6171e-02,  2.9688e-02,\n          -6.3169e-02,  9.2161e-02, -5.1928e-02, -5.1464e-02, -7.5767e-02,\n          -4.8034e-02, -8.7280e-03,  1.0612e-01,  5.8273e-02,  1.7628e-01,\n           3.9572e-02, -2.7251e-01,  2.1254e-02,  5.8274e-02,  3.4897e-02,\n          -1.4095e-02,  4.3540e-02, -1.6112e-01, -3.1265e-01,  1.2687e-01,\n          -1.0584e-01,  4.8220e-02, -3.5159e-02, -6.3323e-03, -4.4044e-02,\n           9.0231e-02,  1.0078e-01, -1.3781e-02,  6.4925e-03,  1.1105e-01,\n          -3.0736e-02, -1.2737e-01,  1.2872e-01, -1.3313e-01, -8.7272e-02,\n          -4.4133e-02, -4.2017e-02,  9.4210e-02,  3.1117e-02, -3.3792e-02,\n           9.2375e-02,  6.9251e-03, -6.7220e-02, -1.9436e-01,  7.4621e-02,\n           2.8375e-02,  3.4200e-02,  5.9312e-02,  6.1955e-02, -1.3118e-02,\n           6.0952e-03,  6.7463e-02,  4.1452e-02,  1.0851e-01,  6.8908e-02,\n           1.3803e-01,  1.3397e-01, -2.7899e-02, -1.3267e-01, -6.6693e-02,\n           8.2437e-03, -1.3131e-01, -1.4711e-02, -4.7202e-02,  1.4308e-02,\n          -1.1165e-01,  9.2159e-02,  5.4878e-02, -1.1633e-01,  9.0942e-02,\n           2.2215e-02, -7.4734e-02, -7.4899e-02, -1.4075e-01, -6.9529e-02,\n           1.6080e-02, -4.1922e-02, -6.9304e-02,  1.1786e-01, -1.0773e-01,\n          -9.8965e-02,  4.5136e-02, -6.3771e-02, -1.9789e-02,  1.0580e-01,\n           9.3797e-03, -2.0742e-02, -9.4794e-02, -2.0453e-02,  7.9776e-03,\n          -1.5836e-01,  5.9475e-02,  2.5717e-01,  1.4948e-01, -7.1241e-02,\n          -7.6345e-02,  1.0665e-01,  6.2769e-03, -9.5808e-04,  8.3594e-03,\n           9.6844e-02, -5.0092e-02, -6.0081e-03, -9.1690e-02,  3.9808e-02,\n           1.6130e-02, -9.1240e-02, -1.1451e-01, -5.1275e-02,  6.3085e-02,\n          -2.0205e-01,  4.0152e-02,  3.0491e-02, -2.9433e-02, -2.4246e-03,\n           8.4275e-02,  1.0313e-01, -1.3940e-01, -9.0754e-03, -6.0386e-02,\n           3.2420e-02,  9.0671e-02, -1.5346e-01,  5.9626e-02, -5.5163e-02,\n           2.0371e-01, -7.9673e-03, -1.3975e-03, -3.5860e-03,  1.8375e-01,\n           2.0368e-01, -4.4980e-02, -6.0884e-02,  7.2809e-02,  1.1396e-02,\n           8.9611e-02, -1.4257e-01,  1.1810e-01,  9.4357e-02, -1.1300e-01,\n           3.2525e-02, -9.9009e-02,  6.0187e-02, -4.6019e-02,  1.0000e-01,\n          -3.2061e-02, -2.6500e-02,  4.0805e-02, -1.4180e-01, -1.2062e-01,\n          -1.7448e-01, -1.1541e-01, -1.2027e-01, -6.5588e-02, -1.6889e-01,\n          -5.1924e-02, -7.4421e-02, -1.9816e-02, -1.8230e-01, -1.3322e-02,\n           5.5806e-03,  6.2261e-02,  7.6494e-02,  9.0945e-02, -1.1985e-01,\n          -1.9771e-01,  1.3663e-03,  1.4724e-01, -3.4526e-02, -6.4695e-03,\n          -1.7027e-02, -8.0658e-04,  1.0157e-01,  5.2106e-04, -6.6904e-02,\n           2.7477e-02,  2.2144e-02,  1.5146e-01, -3.5678e-02, -1.5067e-01,\n          -4.6744e-02,  9.9343e-02,  1.7906e-01, -1.0366e-01, -7.4346e-02,\n           7.6834e-02,  1.0330e-01,  7.8116e-02,  5.1444e-02,  7.9239e-02],\n         [-1.6611e-02, -1.3764e-01,  6.3780e-03,  3.1387e-02, -1.0463e-01,\n          -2.1077e-01, -1.0176e-02, -7.7696e-02,  6.8045e-02,  1.9929e-02,\n          -3.7854e-02, -4.6287e-02,  7.1118e-03, -5.0051e-02, -1.0051e-01,\n          -1.9851e-03,  8.5236e-02,  3.5759e-02,  5.8642e-02,  6.3806e-02,\n           1.0050e-01, -6.8853e-02, -5.2763e-02,  9.0479e-03, -7.7214e-03,\n           5.0449e-03,  1.6053e-01, -1.3111e-01,  9.7474e-02,  8.0425e-03,\n           9.7893e-02,  9.7385e-02,  3.6424e-03, -9.0407e-02,  2.2675e-02,\n          -1.0229e-01,  1.0212e-01, -7.6370e-02, -7.4943e-02, -3.2500e-02,\n          -7.2247e-02,  4.5920e-02,  6.9464e-02,  6.9211e-02,  1.6947e-01,\n           1.7366e-02, -2.3900e-01,  1.1979e-02,  5.8207e-02,  2.7773e-02,\n           3.9336e-03,  6.1977e-02, -1.4290e-01, -3.4311e-01,  1.4508e-01,\n          -1.0283e-01,  5.5988e-02, -4.5516e-02,  7.9728e-03, -6.4397e-02,\n           1.0930e-01,  6.5372e-02,  1.4475e-02, -1.0895e-02,  1.1874e-01,\n          -6.4205e-02, -1.4029e-01,  1.1255e-01, -1.1040e-01, -1.2740e-01,\n          -6.9871e-02, -8.1378e-02,  1.3026e-01,  5.6278e-03, -2.5432e-02,\n           1.0895e-01,  9.7943e-03, -6.2638e-02, -1.3558e-01,  3.8140e-02,\n           2.1813e-02, -1.1709e-02,  8.0051e-02,  6.2655e-02, -1.8201e-02,\n           3.5330e-02,  5.0587e-02,  3.9184e-02,  1.1538e-01,  6.7513e-02,\n           1.2342e-01,  1.7911e-01, -4.6168e-02, -1.9129e-01, -6.6930e-02,\n          -4.1296e-02, -1.2011e-01, -7.4081e-02, -5.9167e-02,  4.6866e-02,\n          -8.4284e-02,  4.1608e-02,  7.4332e-02, -1.6002e-01,  8.0583e-02,\n          -2.9275e-04, -4.1467e-02, -6.9336e-02, -1.5921e-01, -1.0562e-01,\n           3.1637e-02, -8.7393e-02,  7.9663e-03,  1.7723e-01, -1.0171e-01,\n          -1.4055e-01,  1.1106e-03, -4.6027e-02, -3.5446e-02,  1.5855e-01,\n           5.4597e-02,  5.0255e-03, -8.0037e-02, -4.3874e-02,  2.7329e-02,\n          -1.5877e-01,  7.3101e-02,  2.9035e-01,  1.5711e-01, -1.1229e-01,\n          -1.0209e-01,  6.2163e-02,  2.8821e-02,  2.7776e-02,  1.4456e-02,\n           7.4708e-02, -4.2324e-02,  5.2735e-02, -6.2404e-02,  9.9555e-02,\n           6.5727e-02, -8.2398e-02, -9.5341e-02, -2.7479e-02,  5.4341e-02,\n          -1.5790e-01,  3.2706e-02,  5.8737e-02, -4.5621e-02,  2.2431e-02,\n           1.2255e-01,  6.6423e-02, -1.9049e-01,  3.1127e-02, -5.9897e-02,\n           7.1999e-02,  6.7646e-02, -1.5502e-01,  1.1234e-01, -4.6052e-02,\n           1.8329e-01,  5.2490e-02, -9.9773e-03,  3.7217e-02,  1.4194e-01,\n           2.6716e-01, -1.9831e-03, -5.1413e-02,  3.8795e-02, -5.6966e-03,\n           1.4442e-01, -1.0974e-01,  1.4580e-01,  1.0908e-01, -1.1263e-01,\n          -3.0781e-02, -3.8988e-02,  6.3377e-02, -2.6513e-02,  1.0254e-01,\n          -2.1740e-02, -4.3416e-02,  2.9034e-02, -1.5180e-01, -9.6839e-02,\n          -1.7457e-01, -1.6671e-01, -9.8086e-02, -8.5115e-02, -1.8987e-01,\n           5.2290e-03, -6.5634e-02,  4.0620e-03, -1.4311e-01,  1.5891e-02,\n           2.7540e-02,  6.7067e-02,  1.7081e-02,  6.5096e-02, -1.3611e-01,\n          -2.2411e-01,  8.2008e-02,  1.1119e-01,  2.9436e-02, -9.0809e-02,\n          -4.0741e-02, -8.3714e-03,  1.8027e-01, -4.3881e-02, -2.5060e-02,\n           4.7697e-02,  7.0419e-04,  1.5553e-01, -1.8295e-02, -2.3769e-01,\n          -8.5048e-02,  7.9343e-02,  1.2911e-01, -4.0654e-02, -6.6174e-02,\n           9.5911e-02,  1.2902e-01,  2.1061e-02,  4.6500e-02,  1.1785e-01]]],\n       grad_fn=<TransposeBackward0>)\n\n\nHidden state index 0 & index 1 shape (the same): torch.Size([1, 1, 225]) - torch.Size([1, 1, 225])\n\nHidden state index 0 tensor:\ntensor([[[-1.6611e-02, -1.3764e-01,  6.3780e-03,  3.1387e-02, -1.0463e-01,\n          -2.1077e-01, -1.0176e-02, -7.7696e-02,  6.8045e-02,  1.9929e-02,\n          -3.7854e-02, -4.6287e-02,  7.1118e-03, -5.0051e-02, -1.0051e-01,\n          -1.9851e-03,  8.5236e-02,  3.5759e-02,  5.8642e-02,  6.3806e-02,\n           1.0050e-01, -6.8853e-02, -5.2763e-02,  9.0479e-03, -7.7214e-03,\n           5.0449e-03,  1.6053e-01, -1.3111e-01,  9.7474e-02,  8.0425e-03,\n           9.7893e-02,  9.7385e-02,  3.6424e-03, -9.0407e-02,  2.2675e-02,\n          -1.0229e-01,  1.0212e-01, -7.6370e-02, -7.4943e-02, -3.2500e-02,\n          -7.2247e-02,  4.5920e-02,  6.9464e-02,  6.9211e-02,  1.6947e-01,\n           1.7366e-02, -2.3900e-01,  1.1979e-02,  5.8207e-02,  2.7773e-02,\n           3.9336e-03,  6.1977e-02, -1.4290e-01, -3.4311e-01,  1.4508e-01,\n          -1.0283e-01,  5.5988e-02, -4.5516e-02,  7.9728e-03, -6.4397e-02,\n           1.0930e-01,  6.5372e-02,  1.4475e-02, -1.0895e-02,  1.1874e-01,\n          -6.4205e-02, -1.4029e-01,  1.1255e-01, -1.1040e-01, -1.2740e-01,\n          -6.9871e-02, -8.1378e-02,  1.3026e-01,  5.6278e-03, -2.5432e-02,\n           1.0895e-01,  9.7943e-03, -6.2638e-02, -1.3558e-01,  3.8140e-02,\n           2.1813e-02, -1.1709e-02,  8.0051e-02,  6.2655e-02, -1.8201e-02,\n           3.5330e-02,  5.0587e-02,  3.9184e-02,  1.1538e-01,  6.7513e-02,\n           1.2342e-01,  1.7911e-01, -4.6168e-02, -1.9129e-01, -6.6930e-02,\n          -4.1296e-02, -1.2011e-01, -7.4081e-02, -5.9167e-02,  4.6866e-02,\n          -8.4284e-02,  4.1608e-02,  7.4332e-02, -1.6002e-01,  8.0583e-02,\n          -2.9275e-04, -4.1467e-02, -6.9336e-02, -1.5921e-01, -1.0562e-01,\n           3.1637e-02, -8.7393e-02,  7.9663e-03,  1.7723e-01, -1.0171e-01,\n          -1.4055e-01,  1.1106e-03, -4.6027e-02, -3.5446e-02,  1.5855e-01,\n           5.4597e-02,  5.0255e-03, -8.0037e-02, -4.3874e-02,  2.7329e-02,\n          -1.5877e-01,  7.3101e-02,  2.9035e-01,  1.5711e-01, -1.1229e-01,\n          -1.0209e-01,  6.2163e-02,  2.8821e-02,  2.7776e-02,  1.4456e-02,\n           7.4708e-02, -4.2324e-02,  5.2735e-02, -6.2404e-02,  9.9555e-02,\n           6.5727e-02, -8.2398e-02, -9.5341e-02, -2.7479e-02,  5.4341e-02,\n          -1.5790e-01,  3.2706e-02,  5.8737e-02, -4.5621e-02,  2.2431e-02,\n           1.2255e-01,  6.6423e-02, -1.9049e-01,  3.1127e-02, -5.9897e-02,\n           7.1999e-02,  6.7646e-02, -1.5502e-01,  1.1234e-01, -4.6052e-02,\n           1.8329e-01,  5.2490e-02, -9.9773e-03,  3.7217e-02,  1.4194e-01,\n           2.6716e-01, -1.9831e-03, -5.1413e-02,  3.8795e-02, -5.6966e-03,\n           1.4442e-01, -1.0974e-01,  1.4580e-01,  1.0908e-01, -1.1263e-01,\n          -3.0781e-02, -3.8988e-02,  6.3377e-02, -2.6513e-02,  1.0254e-01,\n          -2.1740e-02, -4.3416e-02,  2.9034e-02, -1.5180e-01, -9.6839e-02,\n          -1.7457e-01, -1.6671e-01, -9.8086e-02, -8.5115e-02, -1.8987e-01,\n           5.2290e-03, -6.5634e-02,  4.0620e-03, -1.4311e-01,  1.5891e-02,\n           2.7540e-02,  6.7067e-02,  1.7081e-02,  6.5096e-02, -1.3611e-01,\n          -2.2411e-01,  8.2008e-02,  1.1119e-01,  2.9436e-02, -9.0809e-02,\n          -4.0741e-02, -8.3714e-03,  1.8027e-01, -4.3881e-02, -2.5060e-02,\n           4.7697e-02,  7.0419e-04,  1.5553e-01, -1.8295e-02, -2.3769e-01,\n          -8.5048e-02,  7.9343e-02,  1.2911e-01, -4.0654e-02, -6.6174e-02,\n           9.5911e-02,  1.2902e-01,  2.1061e-02,  4.6500e-02,  1.1785e-01]]],\n       grad_fn=<StackBackward0>)\n\nHidden state index 1 tensor:\ntensor([[[-4.0311e-02, -2.5496e-01,  1.3967e-02,  5.7239e-02, -2.4376e-01,\n          -4.2157e-01, -2.2221e-02, -1.6965e-01,  1.4778e-01,  3.5916e-02,\n          -9.1785e-02, -1.0685e-01,  1.2386e-02, -1.1094e-01, -1.9365e-01,\n          -3.2885e-03,  1.4947e-01,  6.4162e-02,  1.1329e-01,  1.1292e-01,\n           1.7938e-01, -1.1116e-01, -1.2784e-01,  1.7156e-02, -1.5379e-02,\n           1.5020e-02,  4.1599e-01, -2.8935e-01,  1.9521e-01,  1.6729e-02,\n           2.1490e-01,  1.9956e-01,  8.0491e-03, -2.1650e-01,  4.4624e-02,\n          -2.2282e-01,  2.5417e-01, -1.5059e-01, -1.3937e-01, -6.8332e-02,\n          -1.4405e-01,  1.0483e-01,  1.1350e-01,  1.3895e-01,  3.3403e-01,\n           3.3689e-02, -4.7347e-01,  2.5561e-02,  1.1632e-01,  6.2450e-02,\n           7.7533e-03,  1.1743e-01, -2.8935e-01, -5.8655e-01,  3.4119e-01,\n          -1.9934e-01,  1.1436e-01, -1.0904e-01,  1.1869e-02, -1.1786e-01,\n           2.2472e-01,  1.1604e-01,  3.6966e-02, -1.7999e-02,  2.1843e-01,\n          -1.3181e-01, -2.5483e-01,  2.1477e-01, -1.9827e-01, -2.3230e-01,\n          -1.3572e-01, -1.4308e-01,  3.0563e-01,  1.0379e-02, -5.3685e-02,\n           2.9804e-01,  2.2522e-02, -1.0846e-01, -2.5991e-01,  6.5077e-02,\n           4.3197e-02, -2.5381e-02,  1.4622e-01,  1.3586e-01, -3.8728e-02,\n           6.7845e-02,  9.7567e-02,  7.8366e-02,  2.5784e-01,  1.4191e-01,\n           2.7583e-01,  3.7909e-01, -9.8945e-02, -4.2781e-01, -1.2318e-01,\n          -8.1564e-02, -2.8124e-01, -1.3373e-01, -1.1979e-01,  8.3864e-02,\n          -1.7579e-01,  8.9653e-02,  1.3418e-01, -3.4157e-01,  1.4916e-01,\n          -5.3814e-04, -9.8559e-02, -1.4126e-01, -3.3643e-01, -2.1041e-01,\n           5.8376e-02, -1.7290e-01,  1.9569e-02,  3.5803e-01, -2.4930e-01,\n          -3.0356e-01,  2.3910e-03, -9.8150e-02, -6.4477e-02,  3.5235e-01,\n           9.4218e-02,  9.7775e-03, -1.4829e-01, -8.7927e-02,  5.1603e-02,\n          -2.6519e-01,  1.4555e-01,  6.2164e-01,  2.9848e-01, -2.4839e-01,\n          -1.5793e-01,  1.1698e-01,  5.0780e-02,  5.9728e-02,  3.1501e-02,\n           1.4575e-01, -8.8141e-02,  1.2833e-01, -1.1921e-01,  2.0971e-01,\n           1.3323e-01, -1.9687e-01, -2.0891e-01, -6.3982e-02,  9.9391e-02,\n          -3.1358e-01,  6.5028e-02,  1.3689e-01, -8.2493e-02,  4.3509e-02,\n           2.0452e-01,  1.4511e-01, -3.5997e-01,  7.4768e-02, -1.2857e-01,\n           1.7332e-01,  1.4438e-01, -3.6459e-01,  2.8605e-01, -1.0989e-01,\n           4.8863e-01,  9.2185e-02, -2.0110e-02,  8.7631e-02,  2.8276e-01,\n           5.3168e-01, -4.3850e-03, -1.8076e-01,  7.4936e-02, -1.3986e-02,\n           2.8109e-01, -2.3848e-01,  3.0600e-01,  2.3918e-01, -2.2871e-01,\n          -7.2443e-02, -7.3361e-02,  1.2421e-01, -6.0454e-02,  2.2943e-01,\n          -4.2671e-02, -1.0562e-01,  6.3748e-02, -3.2270e-01, -2.1624e-01,\n          -3.5755e-01, -3.3638e-01, -1.8353e-01, -1.5210e-01, -3.8746e-01,\n           1.0406e-02, -1.1648e-01,  7.8334e-03, -2.6368e-01,  2.7003e-02,\n           5.4176e-02,  1.3584e-01,  3.5791e-02,  1.2394e-01, -2.9259e-01,\n          -4.4472e-01,  1.4185e-01,  2.4442e-01,  4.9830e-02, -1.6367e-01,\n          -8.2518e-02, -1.7143e-02,  3.5001e-01, -8.2292e-02, -4.7979e-02,\n           1.1669e-01,  1.3628e-03,  3.2175e-01, -4.2056e-02, -3.8831e-01,\n          -1.7939e-01,  1.3519e-01,  2.6032e-01, -7.5203e-02, -1.2948e-01,\n           1.9466e-01,  2.8884e-01,  4.5751e-02,  1.0295e-01,  2.0272e-01]]],\n       grad_fn=<StackBackward0>)\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"''' Getting values from output using smaller lstm.\n    \n    Remember input to lstm is (input size, hidden dimensions, num layers), and\n    also batch_first if it helps. Hidden dimensions caused the last lstm layer to\n    output a hidden tuple of big values unfortunately.\n    \n    Input to the model must be in (batch_size, seq_len, input size). Create that\n    input with randint\n\n    \n'''\n\nnew_input = 5\nnew_hidden_dims = 9\nnew_layers = 1\nnew_lstm_layer = nn.LSTM(new_input, new_hidden_dims, new_layers, batch_first=True)\n\nx = torch.randint(0, 10, (batch_size, seq_len, new_input), dtype=torch.float32)\n\n# Boolean true, returns tuple of output, hidden.\ny = layer_results(x, new_lstm_layer, return_output=True)\n\n# Get cell state/output\nout = y[0]\n\n# Squeeze, when dealing with a shape like 1,1,9, gives a shape of torch.Size([9])\nprint(out.squeeze().shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:02:06.571253Z","iopub.execute_input":"2024-04-14T14:02:06.571572Z","iopub.status.idle":"2024-04-14T14:02:06.583933Z","shell.execute_reply.started":"2024-04-14T14:02:06.571545Z","shell.execute_reply":"2024-04-14T14:02:06.582994Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Tensor input shape: torch.Size([1, 1, 5])\nTensor:\ntensor([[[1., 0., 7., 1., 3.]]])\n\n\nOutput shape: torch.Size([1, 1, 9])\nOutput tensor:\ntensor([[[ 0.2105, -0.0156,  0.0752,  0.1204, -0.4791, -0.0492,  0.0388,\n          -0.1745, -0.5316]]], grad_fn=<TransposeBackward0>)\n\n\nHidden state index 0 & index 1 shape (the same): torch.Size([1, 1, 9]) - torch.Size([1, 1, 9])\n\nHidden state index 0 tensor:\ntensor([[[ 0.2105, -0.0156,  0.0752,  0.1204, -0.4791, -0.0492,  0.0388,\n          -0.1745, -0.5316]]], grad_fn=<StackBackward0>)\n\nHidden state index 1 tensor:\ntensor([[[ 0.6111, -0.1876,  0.2531,  0.2927, -0.5625, -0.2116,  0.3843,\n          -0.6492, -0.8737]]], grad_fn=<StackBackward0>)\n\n\ntorch.Size([9])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 9) Model building","metadata":{}},{"cell_type":"code","source":" ''' Issue occured where training acc kept rising, yet validation accuracy rose a bit then dropped.\n    This is overfitting. To solve, L1 regularization is used. It gets the sum of the absolute values\n    of ALL the weights to add to the cost. This is better than L2 because I'd like the weights to\n    be smaller. L2 adds the sum of the SQUARED values of all the weights, to the loss. It'll help any\n    negative weights be positive but for current positive weights it'll likely \"explode\" them in a way. \n    \n    Both regularizations, l1 and l2 are scaled small factor/lambda to control their impacts/effects. \n    Normally called \"alpha\" in most cases. '''\n\n\nlearning_rate = 0.001\n\n''' Vital, testing alpha/lambda value to see a change in validation acc/loss. Using extreme values.\n    Both l1/l2 needs alpha argument. Determines how much attention to pay to the penalty of regularization. \n    Penalty  strong? You'll regularize too hard. Weights will be small, underfitting happens. Penalty soft? \n    You won't regularize much, weights will go unpunished and overfitting still occurs.\n\n    Previous values used: 0.01\n    Current - 50 \n    \n    0s array below will keep track of the losses for every epoch of course.\n            \n'''\nl1_lambda = 0.01\n# losses = np.zeros(len(train_loader))","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:02:06.585042Z","iopub.execute_input":"2024-04-14T14:02:06.585353Z","iopub.status.idle":"2024-04-14T14:02:06.591595Z","shell.execute_reply.started":"2024-04-14T14:02:06.585325Z","shell.execute_reply":"2024-04-14T14:02:06.590781Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\n\n\n\nclass LSTMModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, max_padding_len, num_classes):\n        super().__init__()        \n        ''' Embedding layer gets tensor of n size for each word in vocab. Ex: if embedding_dim is 400 and\n            there's 20 words in the vocab, each word gets a 400 length tensor. '''\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        \n        # \n        # self.dropout = nn.Dropout(0.05)\n        \n        ''' Output of embedding is input to lstm. Same concept for all layers.\n            Also, Lstm cells/layers return 2 values. \n            1) cell state (sometimes called \"out\") \n            2) hidden state.\n            Ex: If embedding dim is 100, that's the embedding out, so lstm will take that size\n                as input and things work because each numerical sentence is that size as well. The hidden\n                dimension, 128 as of now, is the size of both the cell state and hidden state. Since input_size\n                to lstm is embedding dim, 100 as of now, that means input to lstm MUST be in shape 1,1,100.\n        '''\n        self.lstm = nn.LSTM(embedding_dim, 128, batch_first=True)\n        \n        # Takes output of previous layer and maps to number of classes to predict.\n        self.fc = nn.Linear(128, num_classes)\n        \n        \n\n    ''' Ex input/x:\n    \n        1) Input/x:\n            \"tensor([[ 4980, 14092,   163,     3,  1015,   638,   757,   922,     3,   205,\n                         1,  3163,   961,   143, 28656,  3043, 14219,     0,     0,     0,\n                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n                         0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\"\n                         \n                This is just a single example. The input is normally batches so forward function expects\n                many numerical inputs together as a 2d matrix. It was MADE 2d with unsqueeze(0) whichs ADDS\n                a dimension horizontally. With that said, shape of above matrix is 1,100. 1 row, 100 columns.\n                100 is max_padding_len to get all inputs padded to same size if numerical sentence is shorter\n                than that. If longer? It's truncated. It goes into the embedding layer and the layer returns a \n                shape of 1,100,100. Why first 100? That's how many numbers there are in each input/x. Why second\n                100? Because thats the embedding dimensions, a tensor/vector/numerical representation for\n                EACH word IN the input tensor. I believe I have a comment explaining this in previous section 8.2.\n                \n                    For this layer:\n                    1) Input - 1,100. 2d\n                    2) Output - 1,100,100. 3d\n                    \n            2) Lstm\n                Just takes the matrix in. The default arguments listed at link below: \n                https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n                This lstm expects input of 100 (the 2nd 100). Earlier I stated \"that means input to lstm MUST \n                be in shape (1,1,t_embedding_dim) which is 3d. embedding dim is 100? Then (1,1,100)\" Of course\n                this is a bit out of context due to the quote saying \"(1,1)\" but the point is, it expects a shape\n                of (#,#,size) to be ITS input which is (#,#,size).\n                \n                128 - As declared above in the lstm declarations, this will be the size of the output, cell state AND hidden\n                    state dimensions in the form (#,#,128)\n                    \n                    For this layer:\n                    1) Input - 1,100,100\n                    2) Cell state & hidden state in same shape: 1,1,128 \n                \n            \n            3) Slicing\n                Ex: If we have the following tensor in variable \"x\":\n                    \"tensor([[[-0.0580, -0.0638,  0.0958,  ..., -0.0865,  0.0669, -0.1144],\n                              [ 0.1291, -0.1556, -0.0057,  ..., -0.2372, -0.0844, -0.0773],\n                              [ 0.0248,  0.0890,  0.1859,  ..., -0.1355, -0.0388, -0.0328],\n                              ...,\n                              [-0.0844,  0.0397,  0.1160,  ..., -0.2957,  0.0158,  0.0983],\n                              [-0.0844,  0.0397,  0.1160,  ..., -0.2957,  0.0158,  0.0983],\n                              [-0.0844,  0.0397,  0.1160,  ..., -0.2957,  0.0158,  0.0983]]],\"\n                              \n                    For the record the size is #,embedding_dim(100),hidden_dim(128)\n                    \n                    : - means to get ALL of something, can be rows or columns\n                    \n                    -1 - In this case, it'd be a row. Negative indexing starts from the last, so this gets the last\n                            row above which is:\n                            \"tensor([[-0.0844,  0.0397,  0.1160,  0.3496,  0.2638,  0.2273, -0.0387,  0.1998,\n                                      ........................more numbers for columns.......................\n                                      0.1347,  0.0985, -0.0176, -0.0115,  0.1013, -0.2957,  0.0158,  0.0983]],\"\n                            It's shape is 1,128. 1 row and 128 columns. \n                            \n                    : - Again, gets all the values.\n                            \n                    \n                    For this operation:\n                    1) Input - 1,1,128 (cell & hidden state)\n                    2) Output - 1,128 \n                    \n            \n            4) Fully connected layer\n                This will take n values and plug each into each neuron that represents num of classes to predict. \n                Number of classes 4? Then each of the 128 neurons in previous are connected to EACH one.\n                \n                    For this layer:\n                    1) Input - 1,128\n                    2) Output - 1,4, four is num classes of course. Ex: tensor([[-0.1676, -0.0530, -0.1906,  0.0272]] '''\n    def forward(self, x):            \n        x = self.embedding(x)\n        # x = self.dropout(x)\n        x, hidden = self.lstm(x)\n        x = x[:, -1, :]\n        x = self.fc(x)\n        return x\n    \n    \n\nlstm_model = LSTMModel(vocab_size=len(vocab.get_stoi()) + 1,\n                       embedding_dim=200,\n                       max_padding_len=max_padding_len,\n                       num_classes=5).to(device) #num classes to 5 now.\n\n\ncriterion = nn.CrossEntropyLoss()  \noptimizer = torch.optim.Adam(lstm_model.parameters(), lr=learning_rate) ","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:02:06.592960Z","iopub.execute_input":"2024-04-14T14:02:06.593234Z","iopub.status.idle":"2024-04-14T14:02:12.274828Z","shell.execute_reply.started":"2024-04-14T14:02:06.593214Z","shell.execute_reply":"2024-04-14T14:02:12.273861Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"## 9.2) Dummy input testing","metadata":{}},{"cell_type":"code","source":"# Test sentence.\ntest_inp = train_data[0]\nprint(f'Padded numerical test input:\\n{test_inp}\\n\\n')\n\n\n''' Remove padding to starting getting back to original sentence. torch.nonzero gets indices \n    of just that. '''\nindices_of_vocab_nums = torch.nonzero(test_inp[0])\nprint(f'All indices of tensor that has non 0 values:\\n{indices_of_vocab_nums.reshape(1, -1)}\\n\\n')\n\n\n''' Only need last value. Squeeze drops the dimensions from 2d to 1d. Then negative index to \n    get last one. .item() just gets the number out. '''\nindex = indices_of_vocab_nums.squeeze()[-1].item()\nprint(f'Index of last vocab word: {index}\\n\\n')\n\n\n# Get numerical sentence. test_inp is tuple, as seen below.\nnum_sent = test_inp[0][:index+1]\nprint(f'Numerical sentence:\\n{num_sent}\\n\\n')\n\n\n# Use vocab object to convert to original\ntext_sent = ' '.join([vocab.get_itos()[num] for num in num_sent])\nprint(f'Real text sentence:\\n{text_sent}\\n\\n')\n\n\n''' Could of also used x_train_sequences since it contains the original nums, TO CONVERT TO SHOW, \n    as shown below. But above was a slightly different challenge regarding getting rid of padding. '''\nz = ' '.join([vocab.get_itos()[num] for num in x_train_sequences[0]])\nprint(f'Same sentence converted (but from x_train_sequences):\\n{z}')","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:02:12.276077Z","iopub.execute_input":"2024-04-14T14:02:12.276638Z","iopub.status.idle":"2024-04-14T14:02:59.093757Z","shell.execute_reply.started":"2024-04-14T14:02:12.276605Z","shell.execute_reply":"2024-04-14T14:02:59.092746Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"Padded numerical test input:\n(tensor([   743,      3,    147,    379,     43,    582,    988,   2965,      2,\n           931,   2094,   1096,     17,     13,     97,   7374,      2,     86,\n           114,      5,    112,      5,     21,    327,      1,    466,     12,\n             4,  15040,    202,   6500,     10,     13,    109,    704,    163,\n          7449,     13,    296,      7,      1,     31,    202,   6500,   3421,\n            13,     97,   7374,      6,     26,    559,      4,     80,     22,\n             1,   1380,      5,    347,    251,      4,   1018,   2301, 591609,\n             6,    159,      2,    557,     23,    410,   1214,    471,     75,\n           156,     15,     24,   2965,     36,      5,    145,    117,     15,\n            24,    533,      2,     37,     15,    412,    247,     81,    889,\n            62,   5662,     10,      3,   2094,    704,     12,    142,    519,\n             2,     24,   4859,    598,      2,    325, 663419,    159,   1502,\n           410,     78,   1808,     17, 524549,   2010,    308,     86,     21,\n            58,   3973,     14,     86,    102,    162,     13,   2093,     27,\n           538,    375,    514,   2093,      2,      8,    160,     34,      3,\n            30,   1387,    137,      5,   2673,    139,      3,   2010,     17,\n           309,    218, 491348,     97,   3710,      9,   2965,      4,    236,\n          1449,      9,    120,      2,      3,    320,   1096,   1278,     71,\n           394,      5,    493,    901,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0]), 3)\n\n\nAll indices of tensor that has non 0 values:\ntensor([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n          14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n          28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n          42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n          56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n          70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n          84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n          98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n         112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n         126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n         140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n         154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165]])\n\n\nIndex of last vocab word: 165\n\n\nNumerical sentence:\ntensor([   743,      3,    147,    379,     43,    582,    988,   2965,      2,\n           931,   2094,   1096,     17,     13,     97,   7374,      2,     86,\n           114,      5,    112,      5,     21,    327,      1,    466,     12,\n             4,  15040,    202,   6500,     10,     13,    109,    704,    163,\n          7449,     13,    296,      7,      1,     31,    202,   6500,   3421,\n            13,     97,   7374,      6,     26,    559,      4,     80,     22,\n             1,   1380,      5,    347,    251,      4,   1018,   2301, 591609,\n             6,    159,      2,    557,     23,    410,   1214,    471,     75,\n           156,     15,     24,   2965,     36,      5,    145,    117,     15,\n            24,    533,      2,     37,     15,    412,    247,     81,    889,\n            62,   5662,     10,      3,   2094,    704,     12,    142,    519,\n             2,     24,   4859,    598,      2,    325, 663419,    159,   1502,\n           410,     78,   1808,     17, 524549,   2010,    308,     86,     21,\n            58,   3973,     14,     86,    102,    162,     13,   2093,     27,\n           538,    375,    514,   2093,      2,      8,    160,     34,      3,\n            30,   1387,    137,      5,   2673,    139,      3,   2010,     17,\n           309,    218, 491348,     97,   3710,      9,   2965,      4,    236,\n          1449,      9,    120,      2,      3,    320,   1096,   1278,     71,\n           394,      5,    493,    901])\n\n\nReal text sentence:\nspent a few hours here saturday afternoon bowling and playing video games with my little bro and im going to try to not let the fact that i bowled 5 strikes in my best game ever affect my review of the place 5 strikes unbelievable my little bro was so impressed i didnt have the heart to tell him i normally suck nngoodnservice was friendly and helpful had several folks stop by while we were bowling just to see how we were doing and if we needed anything also lost some quarters in a video game that wasnt working and were refunded quickly and without questionnfamily friendly environment several other groups with kidsnnbadnlimited ball selection im not an expert but im pretty sure my fingers are regular guy size fingers and it took me a good ten minutes to track down a ball with large enough holesna little pricy for bowling i thought shoes for two and a couple games ran us close to 30 bucks\n\n\nSame sentence converted (but from x_train_sequences):\nspent a few hours here saturday afternoon bowling and playing video games with my little bro and im going to try to not let the fact that i bowled 5 strikes in my best game ever affect my review of the place 5 strikes unbelievable my little bro was so impressed i didnt have the heart to tell him i normally suck nngoodnservice was friendly and helpful had several folks stop by while we were bowling just to see how we were doing and if we needed anything also lost some quarters in a video game that wasnt working and were refunded quickly and without questionnfamily friendly environment several other groups with kidsnnbadnlimited ball selection im not an expert but im pretty sure my fingers are regular guy size fingers and it took me a good ten minutes to track down a ball with large enough holesna little pricy for bowling i thought shoes for two and a couple games ran us close to 30 bucks\n","output_type":"stream"}]},{"cell_type":"code","source":"# Padded input must be 2d. Model takes batches in, 2d represents each numerical sentence. Shape now 1,100\ntest_inp = test_inp[0].unsqueeze(0).to(device)\n\n# Pass the padded version to model\nr = lstm_model(test_inp)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:02:59.099321Z","iopub.execute_input":"2024-04-14T14:02:59.099631Z","iopub.status.idle":"2024-04-14T14:02:59.657989Z","shell.execute_reply.started":"2024-04-14T14:02:59.099609Z","shell.execute_reply":"2024-04-14T14:02:59.657099Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"# 10) Validation & Training function","metadata":{}},{"cell_type":"code","source":"# Passing model and other variables for training/validation is a common practice. Helps organize things in my opinion.\ndef validate(model, val_loader, criterion, log_interval, device='cpu', display=True, return_data_as_lists=False):\n    # Turns off any dropout, batchnorm and layers of the like for predictions.\n    model.eval()\n    \n    total_val_loss = 0.0\n    # Needed for accuracy calculations. Ex: 9 / 10 on a test is 0.9. Times 100 = 90%.\n    correct_val_predictions = 0\n    total_val_samples = 0\n    \n        \n    ''' The default boolean \"return_data_as_lists\" was created for later Optuna use. When a model would be saved, it gets\n        loaded again later and validated as well. Getting the list values that made up the original saved models val loss\n        and val acc is important for extra comparison in graphs with other models. '''\n    val_loss_list = []\n    val_acc_list = []\n    \n\n    # no_grad turns off computation graph to make things faster.\n    with torch.no_grad():\n        # val and train loaders are data loaders. They return numerical text and labels\n        for i, x in enumerate(val_loader):\n            val_labels = None\n            val_sentences = None\n            val_outputs = None\n            \n            # Unpack.\n            val_sentences = x[0]\n            val_labels = x[1]\n                \n            # Place them hopefully on gpu, if it was found earlier.\n            val_sentences = val_sentences.to(device)\n            val_labels = val_labels.to(device)\n            \n            # Predictions. Model will return 2d tensor.\n            val_outputs = model(val_sentences)\n            \n            # Get loss. Loss finds difference between predictions and ground truths \n            val_loss = criterion(val_outputs, val_labels)\n            \n            # Of course increment loss.\n            total_val_loss += val_loss.item()\n            \n            # For \"return_data_as_lists\" default arg\n            val_loss_list.append(total_val_loss / len(val_loader))\n            \n            \n            ''' Simple accuracy calculations.\n            \n                Since train and validation loader batch size is 64 at the time of this comment, that means the return\n                value of torch.max (values & indices), the output shape, labels and the == comparison will all be size\n                64. Steps:\n                \n                    1) torch.max(predictions/outputs/etc name, 1) - This gets the 2d predictions and dim 1 means go\n                        row by row and get the highest value. Function returns tuple of values, and indices. Values are\n                        the actual highest numbers row to row, going in a vertical manner. Indices are just the literal\n                        indices where the highest values are in that row. \n                        Ex:\n                        \"x = t.tensor([[-0.0580, -0.0638,  0.0958, -0.0865,  0.0669, -0.1144],\n                                       [ 0.1291, -0.1556, -0.0057, -0.2372, -0.0844, -0.0773],\n                                       [ 0.0248,  0.0890,  0.1859, -0.1355, -0.0388, -0.0328]])\n                         values, indices = x.max(dim=1)\n                         print(values)\n                         print(indices)\n                        \"\n                        The code prints:\n                        \"tensor([0.0958, 0.1291, 0.1859])\n                         tensor([2, 0, 2])\"\n                         Meaning obviously the first tensor has the highest nums and the 2,0,2 are their indices in the row.\n                        \n                \n                    2) == - The coolest comparison operator yet. The predictions and labels are used like \"predictions == labels\"\n                        and this returns a boolean tensor where True means a match, aka the model got it right. False means the \n                        model didn't get it right. Then .sum() just adds up all the True values.\n                        Ex:\n                        \"([False, False, False, False, False,  True, False, False,  True, False,\n                           True, False,  True, False, False,  True, False, False, False, False,\n                           False, False,  True, False, False, False,  True, False, False, False,\n                           True, False, False, False, False, False, False, False,  True,  True,\n                           False, False, False, False,  True,  True, False, False, False, False,\n                           False, False,  True, False, False,  True, False, False, False, False,\n                           False, False, False, False])\"\n                        This has a total of 14.\n                        then use .item() to get the real value.\n                1\n                    3) labels.size(0) - Take the shape, in this case torch.Size([64]) which is data loader batch size and add to\n                        total. Needed for accuracy. '''            \n            _, predicted_val = torch.max(val_outputs, 1)\n            correct_val_predictions += (predicted_val == val_labels).sum().item()\n            total_val_samples += val_labels.size(0)\n            \n            # For \"return_data_as_lists\" default arg\n            val_acc_list.append((correct_val_predictions / total_val_samples) * 100)        \n            \n#             if i % log_interval == 0 and i > 0 and display is True:\n#                 # Print info to see\n#                 print(\n#                     \"Validation - | {:5d}/{:5d} batches \"\n#                     \"| accuracy {:8.3f}\".format(\n#                         i, len(val_loader), (correct_val_predictions / total_val_samples) * 100\n#                     )\n#                 )\n            \n\n    validation_accuracy = correct_val_predictions / total_val_samples\n    \n    if return_data_as_lists is True:\n        return val_loss_list, val_acc_list\n\n    # Return both  loss and accuracy for graphing purposes. Multi acc by 100 for better value. Ex: 0.9 * 100 = 90.\n    return total_val_loss / len(val_loader), validation_accuracy * 100","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:02:59.659099Z","iopub.execute_input":"2024-04-14T14:02:59.659396Z","iopub.status.idle":"2024-04-14T14:02:59.673550Z","shell.execute_reply.started":"2024-04-14T14:02:59.659372Z","shell.execute_reply":"2024-04-14T14:02:59.672439Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# For graphing.\ntrain_loss = []\ntrain_acc = []\nval_loss = []\nval_acc = []\n\ndef train(model, epochs, train_loader, optimizer, criterion, log_interval=500, scheduler=None, val_loader=None, device='cpu', display=True, \n          use_l1=False, n_weights=0, l1_lambda=0):\n    # Validation and training funcs similar, not too much to explain here.\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0.0\n\n        correct_train_predictions = 0\n        total_train_samples = 0\n            \n        for i, x in enumerate(train_loader):\n            labels = None\n            sentences = None\n            outputs = None\n            \n            sentences = x[0]\n            labels = x[1]\n            \n            sentences = sentences.to(device)\n            labels = labels.to(device)\n                \n            outputs = model(sentences)\n\n            loss = criterion(outputs, labels)\n            \n            \n            ''' If func call requested L1 to be performed, do it here. Create starting var 0 in tensor format to be added to \n                other tensors later. Loop all params and apply l1 math which is get the sum of the absolute values of ALL \n                the weights to add to the cost. Use total weights to divide by the l1 value. Then finally use the alpha/lambda \n                factor on the loss itself. '''\n            if use_l1 is True:\n                torch.cuda.empty_cache()\n                \n                l1_term = torch.tensor(0., requires_grad=True)\n                \n                for name, weights in model.named_parameters():\n                    if 'bias' not in name:\n                        weights_sum = torch.sum(torch.abs(weights))\n                        l1_term = l1_term + weights_sum\n                l1_term = l1_term / n_weights\n                \n                loss = loss - l1_term * l1_lambda\n\n\n\n            ''' Backpropagation done to update weights according to the gradient of the loss func and optimization step\n                updates the weights gradients. How? Ex: Stochastic gradient descent (sgd) - Get the gradients and multiply \n                them by a small factor/learning rate, take the result and subtract it FROM the weights in order to get \n                closer to 0 (or basically minimize the loss) '''\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            if scheduler:\n                # Adjust the learning rate based on the number of epochs\n                scheduler.step()\n\n            total_loss += loss.item()\n\n            values, indices = torch.max(outputs, 1)\n            correct_train_predictions += (indices == labels).sum().item()\n            total_train_samples += labels.size(0)\n            \n#             if i % log_interval == 0 and i > 0 and display is True:\n#                 print(\n#                     \"| i: {:3d} | {:5d}/{:5d} batches \"\n#                     \"| accuracy {:8.3f}\".format(\n#                         epoch+1, i, len(train_loader), (correct_train_predictions / total_train_samples) * 100\n#                     )\n#                 )\n                \n            \n        # Then just simply divide how many were correct with the total to get accuracy\n        training_accuracy = correct_train_predictions / total_train_samples\n\n        if display is True:\n            print(f'Epoch [{epoch + 1}/{epochs}], Training Loss: {total_loss / len(train_loader):.3f}, Training Accuracy: {training_accuracy * 100:.3f}%')\n        \n        \n        # Save training loss & acc\n        train_loss.append(total_loss / len(train_loader))\n        train_acc.append(training_accuracy * 100)\n\n        \n        if val_loader is not None:\n            # Validation step using the callback\n            vali_loss, val_accuracy = validate(model, val_loader, criterion, log_interval, device=device, display=display)\n            if display is True:\n                print(f'Epoch [{epoch + 1}/{epochs}], Validation Loss: {vali_loss:.3f}, Validation Accuracy: {val_accuracy:.3f}%\\n')\n            \n            # Save validation loss & acc\n            val_loss.append(vali_loss)\n            val_acc.append(val_accuracy)\n        \n        if display is True:\n            print('\\n')\n\n    if display is True:\n        print('Training finished')\n    \n    # All models will have their own train acc/loss, val acc/loss, so return them to each.\n    return train_loss, train_acc, val_loss, val_acc","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:02:59.674899Z","iopub.execute_input":"2024-04-14T14:02:59.675506Z","iopub.status.idle":"2024-04-14T14:02:59.694315Z","shell.execute_reply.started":"2024-04-14T14:02:59.675474Z","shell.execute_reply":"2024-04-14T14:02:59.693410Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"# Training a model with this much data DOES take a while, so low epochs recommended.\nepochs = 6\n\nlstm_train_losses, lstm_train_accs, lstm_val_losses, lstm_val_accs = train(model=lstm_model, \n                                                                           epochs=epochs, \n                                                                           train_loader=train_loader, \n                                                                           optimizer=optimizer, \n                                                                           criterion=criterion, \n                                                                           val_loader=val_loader,\n                                                                           device=device)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:02:59.695338Z","iopub.execute_input":"2024-04-14T14:02:59.695603Z","iopub.status.idle":"2024-04-14T14:38:27.417939Z","shell.execute_reply.started":"2024-04-14T14:02:59.695581Z","shell.execute_reply":"2024-04-14T14:38:27.417047Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"Epoch [1/6], Training Loss: 1.158, Training Accuracy: 47.634%\nEpoch [1/6], Validation Loss: 0.934, Validation Accuracy: 59.012%\n\n\n\nEpoch [2/6], Training Loss: 0.873, Training Accuracy: 62.178%\nEpoch [2/6], Validation Loss: 0.899, Validation Accuracy: 60.771%\n\n\n\nEpoch [3/6], Training Loss: 0.776, Training Accuracy: 67.125%\nEpoch [3/6], Validation Loss: 0.909, Validation Accuracy: 60.775%\n\n\n\nEpoch [4/6], Training Loss: 0.674, Training Accuracy: 72.387%\nEpoch [4/6], Validation Loss: 0.967, Validation Accuracy: 60.246%\n\n\n\nEpoch [5/6], Training Loss: 0.568, Training Accuracy: 77.448%\nEpoch [5/6], Validation Loss: 1.059, Validation Accuracy: 59.031%\n\n\n\nEpoch [6/6], Training Loss: 0.471, Training Accuracy: 81.905%\nEpoch [6/6], Validation Loss: 1.195, Validation Accuracy: 58.429%\n\n\n\nTraining finished\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 11) Regularization","metadata":{}},{"cell_type":"code","source":"''' 1) What is regularization?\n       It's reducing overfitting in models. It can decrease training accuracy for an increase in generalization/testing/validation \n       accuracy.\n       \n           1.2) What is overfitting? \n                When the model fits the training data SO closely, it doesn't generalize to the real world data which is in test/valid\n                data. This is happening above.\n                \n    2) Bias & Variance\n       Necessary to talk about because it applies here. Bias & Variance trade offs happen because of increasing training error (lower\n       training accuracy) for lower testing error (higher test/validation accuracy). Bias is the AVERAGE difference between predictions\n       and ground truths. Bias increase? Model predicts less well on training data. So if training acc low or training error/loss high,\n       high bias is what is going on. Variance is how the model predictions change when the given data changes. When variance INCREASES, \n       model predicts not so well on unseen data. Variance increase? Model predicts less well on testing data. So if testing/validation\n       acc low or testing/validation error/loss high, high variance is what is going on. So this model seems to have high variance.\n       It's not always possible to decrease both bias AND variance. Regularization is meant to take care of variance, at the cost of\n       bias. Again, it's a trade off. \n       \n           2.2) Causes of variance\n                1) High weights - In a neural network, of course every neuron has a weight. With high weights, the \"impact\" or\n                    \"importance\" to that weight is highly exaggerated when it shouldn't be. In order to exaggerate, weights\n                    must be high. So I'm running with the reason overfitting is going on is because of high weights.\n                \n    3) Solving regularization - Both below considered solutions add weights into the loss, to effect the network weights. And both\n        are scalred by a factor to control their impact. Called alpha/penalty/lambda. Penalty strong? You'll regularize likely too hard \n        and cause underfitting. Penalty too strong? You'll regularize hardly at all and overfitting still happens.\n    \n        3.1) L1 - Sum up all weights, get their absolute values, add it to the loss.\n        3.2) L2 - Add the sum of squared values of the weights. Makes negative weights positive but will also exaggerate\n                    the effects of higher values. \n                    \n        Conclusion: L1 will be implemented. Why? L2 will make things worse if the weights are high. If variance is the issue,\n            then the weights are ALREADY high. Last thing needed is the squared weights to make them higher.\n           \n    \n    \n    See: \n    1) https://www.ibm.com/topics/regularization \n    2) https://www.youtube.com/watch?v=EehRcPo1M-Q&t=112s\n    For more on regularization.\n\n'''","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:38:27.419576Z","iopub.execute_input":"2024-04-14T14:38:27.420464Z","iopub.status.idle":"2024-04-14T14:38:27.429028Z","shell.execute_reply.started":"2024-04-14T14:38:27.420436Z","shell.execute_reply":"2024-04-14T14:38:27.428131Z"},"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"' 1) What is regularization?\\n       It\\'s reducing overfitting in models. It can decrease training accuracy for an increase in generalization/testing/validation \\n       accuracy.\\n       \\n           1.2) What is overfitting? \\n                When the model fits the training data SO closely, it doesn\\'t generalize to the real world data which is in test/valid\\n                data. This is happening above.\\n                \\n    2) Bias & Variance\\n       Necessary to talk about because it applies here. Bias & Variance trade offs happen because of increasing training error (lower\\n       training accuracy) for lower testing error (higher test/validation accuracy). Bias is the AVERAGE difference between predictions\\n       and ground truths. Bias increase? Model predicts less well on training data. So if training acc low or training error/loss high,\\n       high bias is what is going on. Variance is how the model predictions change when the given data changes. When variance INCREASES, \\n       model predicts not so well on unseen data. Variance increase? Model predicts less well on testing data. So if testing/validation\\n       acc low or testing/validation error/loss high, high variance is what is going on. So this model seems to have high variance.\\n       It\\'s not always possible to decrease both bias AND variance. Regularization is meant to take care of variance, at the cost of\\n       bias. Again, it\\'s a trade off. \\n       \\n           2.2) Causes of variance\\n                1) High weights - In a neural network, of course every neuron has a weight. With high weights, the \"impact\" or\\n                    \"importance\" to that weight is highly exaggerated when it shouldn\\'t be. In order to exaggerate, weights\\n                    must be high. So I\\'m running with the reason overfitting is going on is because of high weights.\\n                \\n    3) Solving regularization - Both below considered solutions add weights into the loss, to effect the network weights. And both\\n        are scalred by a factor to control their impact. Called alpha/penalty/lambda. Penalty strong? You\\'ll regularize likely too hard \\n        and cause underfitting. Penalty too strong? You\\'ll regularize hardly at all and overfitting still happens.\\n    \\n        3.1) L1 - Sum up all weights, get their absolute values, add it to the loss.\\n        3.2) L2 - Add the sum of squared values of the weights. Makes negative weights positive but will also exaggerate\\n                    the effects of higher values. \\n                    \\n        Conclusion: L1 will be implemented. Why? L2 will make things worse if the weights are high. If variance is the issue,\\n            then the weights are ALREADY high. Last thing needed is the squared weights to make them higher.\\n           \\n    \\n    \\n    See: \\n    1) https://www.ibm.com/topics/regularization \\n    2) https://www.youtube.com/watch?v=EehRcPo1M-Q&t=112s\\n    For more on regularization.\\n\\n'"},"metadata":{}}]},{"cell_type":"code","source":"# Create new model for purpose. Loss func will be same, but optimizer needs specific model.\nl1_lstm_model = LSTMModel(vocab_size=len(vocab.get_stoi()) + 1,\n                          embedding_dim=200,\n                          max_padding_len=max_padding_len,\n                          num_classes=5).to(device) #num classes to 5 now.\n\nl1_criterion = nn.CrossEntropyLoss()  \nl1_optimizer = torch.optim.Adam(l1_lstm_model.parameters(), lr=learning_rate) ","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:38:27.430133Z","iopub.execute_input":"2024-04-14T14:38:27.430411Z","iopub.status.idle":"2024-04-14T14:38:30.644353Z","shell.execute_reply.started":"2024-04-14T14:38:27.430375Z","shell.execute_reply":"2024-04-14T14:38:30.643333Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"### Below is from https://www.kaggle.com/code/cheesleypringlesman/minimizing-loss-using-l1-regularization-in-pytorch\n\n''' To use L1 regularization, get num of weights in the model. The var \"weights\" itself contains 2d matrices. Ex:\n    First one is shape (len(vocab), embedding_dim). In this projects case, vocab is 837,415, and current embedding \n    dim is 200. Each iteration will bring a different matrix of a different shape. Numel gets total num of values \n    in the tensor, counting how many weights. '''\n\n# use_l1=False, n_weights=0, l1_lambda=0.01\n\nl1_weights = 0\n\nfor name, weights in l1_lstm_model.named_parameters():\n    if 'bias' not in name:\n        l1_weights = l1_weights + weights.numel()\n        \nprint(f'Total amount of weights in l1_lstm_model: {l1_weights}')","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:38:30.667398Z","iopub.execute_input":"2024-04-14T14:38:30.667685Z","iopub.status.idle":"2024-04-14T14:38:30.673500Z","shell.execute_reply.started":"2024-04-14T14:38:30.667663Z","shell.execute_reply":"2024-04-14T14:38:30.672614Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"Total amount of weights in l1_lstm_model: 167651776\n","output_type":"stream"}]},{"cell_type":"code","source":"''' Train the model that will attempt regularization. Bare in mind, the arguments the training function has at the end\n    of it which are \"use_l1=False, n_weights=0, l1_lambda=0\".\n    \n    1) use_l1 - Do we want to use it?\n    2) n_weights - The weights of the model. Calculated in above cell.\n    3) l1_lambda - The lambda/alpha/penalty to determine how HARD to actually punish the model weights. Remember, too\n                    BIG a value, and the model is punished too much and underfitting occurs. Vice versa? Overfitting\n                    still happens.\n                    \n                    \n                        !!! IMPORTANT !!!\n    I commented out the below l1 training because training TWO models on such a HUGE dataset takes FOREVER. Even with the\n    kaggle gpu that puts my own pcs gpu to shame. So if anyone would like to try a different l1 lambda value, feel free\n    to uncomment the code and put whatever float number you'd like with the variable custom_lambda_val below.\n    '''\n\n# custom_lambda_val = 0.5\n# l1_lstm_train_losses, l1_lstm_train_accs, l1_lstm_val_losses, l1_lstm_val_accs = train(model=l1_lstm_model, \n#                                                                                        epochs=epochs, \n#                                                                                        train_loader=train_loader, \n#                                                                                        optimizer=l1_optimizer, \n#                                                                                        criterion=l1_criterion, \n#                                                                                        val_loader=val_loader,\n#                                                                                        device=device,\n#                                                                                        use_l1=True,\n#                                                                                        n_weights=l1_weights,\n#                                                                                        l1_lambda=custom_lambda_val)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:38:30.675072Z","iopub.execute_input":"2024-04-14T14:38:30.675342Z","iopub.status.idle":"2024-04-14T14:38:30.685893Z","shell.execute_reply.started":"2024-04-14T14:38:30.675321Z","shell.execute_reply":"2024-04-14T14:38:30.684939Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"' Train the model that will attempt regularization. Bare in mind, the arguments the training function has at the end\\n    of it which are \"use_l1=False, n_weights=0, l1_lambda=0\".\\n    \\n    1) use_l1 - Do we want to use it?\\n    2) n_weights - The weights of the model. Calculated in above cell.\\n    3) l1_lambda - The lambda/alpha/penalty to determine how HARD to actually punish the model weights. Remember, too\\n                    BIG a value, and the model is punished too much and underfitting occurs. Vice versa? Overfitting\\n                    still happens.\\n                    \\n                    \\n                        !!! IMPORTANT !!!\\n    I commented out the below l1 training because training TWO models on such a HUGE dataset takes FOREVER. Even with the\\n    kaggle gpu that puts my own pcs gpu to shame. So if anyone would like to try a different l1 lambda value, feel free\\n    to uncomment the code and put whatever float number you\\'d like with the variable custom_lambda_val below.\\n    '"},"metadata":{}}]},{"cell_type":"markdown","source":"# 12) Optuna ","metadata":{}},{"cell_type":"code","source":"''' Optuna is a hyperparameter optimization library that's very useful in doing just that.\n    See here: https://optuna.readthedocs.io/en/stable/\n    \n    A bit of overfitting is happening above with the validation accuracy. So to not waste time experimenting with \n    changing one thing then another (I've already tried this and it definitely wasn't worth much), Optuna\n    will be used. \n    \n    \n                        !!! IMPORTANT !!!\n                        \n    I've used Optuna before in a previous project, and THAT project was born simply to figure out how to\n    solve the overfitting in THIS one. So time to apply it here. \n    Here is that project by the way: https://github.com/axiom2018/AG-News-Classification  '''\n\n!pip install optuna","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:38:30.687071Z","iopub.execute_input":"2024-04-14T14:38:30.687366Z","iopub.status.idle":"2024-04-14T14:38:44.965903Z","shell.execute_reply.started":"2024-04-14T14:38:30.687329Z","shell.execute_reply":"2024-04-14T14:38:44.964696Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Requirement already satisfied: optuna in /opt/conda/lib/python3.10/site-packages (3.5.0)\nRequirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (1.13.1)\nRequirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna) (6.8.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (21.3)\nRequirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from optuna) (2.0.25)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from optuna) (4.66.1)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from optuna) (6.0.1)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.3.2)\nRequirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->optuna) (3.1.1)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n","output_type":"stream"}]},{"cell_type":"code","source":"import optuna","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:38:44.967572Z","iopub.execute_input":"2024-04-14T14:38:44.967906Z","iopub.status.idle":"2024-04-14T14:38:49.921099Z","shell.execute_reply.started":"2024-04-14T14:38:44.967879Z","shell.execute_reply":"2024-04-14T14:38:49.920192Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"''' Need only a portion of data because hyperparameter tuning means multiple models used AND trained. By\n    DEFAULT, this dataset is 650k rows. Kaggle (likely) doesn't have the gpu for that. My account DEFINITELY\n    doesn't have enough gpu hours to utilize. \n    \n    5500 is a bit arbitrary, I selected it because 5500 out of 650k is 0.85% of the dataset. In my previous\n    project the dataset size was 96k and the portion was 750, and that was 0.78%. '''\n\n# portion = 750\n# portion = 5500\nportion = 150\n\n_sample_x_train_padded = x_train_padded[:portion]\n_sample_y_train_np = y_train_np[:portion]\n\n_sample_x_val_padded = x_val_padded[:portion]\n_sample_y_val_np = y_val_np[:portion]\n\nprint(f'Portion size: {portion}\\n')\n\nshow = 2\nprint(f'{portion} in portion. Will show {show} of x_train_padded:\\n{_sample_x_train_padded[:show]}\\n')\nprint(f'{portion} in portion. Will show {show} of y_train_np:\\n{_sample_y_train_np[:show]}\\n')\nprint(f'{portion} in portion. Will show {show} of x_val_padded:\\n{_sample_x_val_padded[:show]}\\n')\nprint(f'{portion} in portion. Will show {show} of y_val_np:\\n{_sample_y_val_np[:show]}\\n')","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:38:49.925225Z","iopub.execute_input":"2024-04-14T14:38:49.925520Z","iopub.status.idle":"2024-04-14T14:38:49.940131Z","shell.execute_reply.started":"2024-04-14T14:38:49.925495Z","shell.execute_reply":"2024-04-14T14:38:49.939190Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"Portion size: 150\n\n150 in portion. Will show 2 of x_train_padded:\n[tensor([   743,      3,    147,    379,     43,    582,    988,   2965,      2,\n           931,   2094,   1096,     17,     13,     97,   7374,      2,     86,\n           114,      5,    112,      5,     21,    327,      1,    466,     12,\n             4,  15040,    202,   6500,     10,     13,    109,    704,    163,\n          7449,     13,    296,      7,      1,     31,    202,   6500,   3421,\n            13,     97,   7374,      6,     26,    559,      4,     80,     22,\n             1,   1380,      5,    347,    251,      4,   1018,   2301, 591609,\n             6,    159,      2,    557,     23,    410,   1214,    471,     75,\n           156,     15,     24,   2965,     36,      5,    145,    117,     15,\n            24,    533,      2,     37,     15,    412,    247,     81,    889,\n            62,   5662,     10,      3,   2094,    704,     12,    142,    519,\n             2,     24,   4859,    598,      2,    325, 663419,    159,   1502,\n           410,     78,   1808,     17, 524549,   2010,    308,     86,     21,\n            58,   3973,     14,     86,    102,    162,     13,   2093,     27,\n           538,    375,    514,   2093,      2,      8,    160,     34,      3,\n            30,   1387,    137,      5,   2673,    139,      3,   2010,     17,\n           309,    218, 491348,     97,   3710,      9,   2965,      4,    236,\n          1449,      9,    120,      2,      3,    320,   1096,   1278,     71,\n           394,      5,    493,    901,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0]), tensor([  8173,   5172,      2,      4,   2172,   1981,     43,    132,   1354,\n            56,      5,      1,  10691,     16,      6,     30,    492,      4,\n           405,   1492,      4,      6,      3, 718813,    321,   3275,     25,\n             1,    705,      7,    276,   1981,     10,   2240,     25,  62949,\n           170,      4,    108,      1,  11414,     24,  28182,    490,     88,\n           860,      2,   2260,      2,      1,    510,    538,   1304,    510,\n             6,    855,     17,      1,    403,    548,      7,   3091,      2,\n         10550,    318,     29,   2723,     43,    302,     50,      4,    279,\n          2240,    170,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0])]\n\n150 in portion. Will show 2 of y_train_np:\n[3 4]\n\n150 in portion. Will show 2 of x_val_padded:\n[tensor([     4,     22,     66,      5,      1,   6466,    182,    172,     18,\n          1577,      1,   5055,  12721,   1251,    368,  57113,     63,     11,\n           970,      3,   2844,      5, 305601,    123,     12,     18,     22,\n           486,    348,      2,    856,   1986,    657,   3077,  42279,     74,\n            42,     40,    678,      2,     40,    774,     14,     12,     36,\n           460,      8,     12,     92,     68, 456595,    214,      1,    535,\n          2022,     14,     22,    214,      1,   1473,   1198,      2,     45,\n           960,     35,      8, 728218,     45,     54,  15478,     26,     19,\n          1901,   5107,      8,    346,   2502,   2463,     14,     19,     69,\n           235,     33,     37,     67, 409166,     38,      1,    690, 122526,\n           307,  10053,   4697,      5,      1,  12721, 184346, 763649,    194,\n        577694,    348,    123,     12,      1,    171,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0,      0,      0,      0,      0,      0,      0,      0,\n             0,      0]), tensor([  21,    1,  109,  105,   23,    1,   53,   11,   21,   30,    1,   28,\n          11,   26,   26,    2,    1, 1496,  463,    5,   29,    3,  176,  339,\n           9,    1,  246,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0])]\n\n150 in portion. Will show 2 of y_val_np:\n[4 2]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"''' Make Datasets of _sample_x_train_padded, _sample_y_train_np, _sample_x_val_padded and _sample_y_val_np \n    before DataLoaders can be made. '''\n\n_train_data = TextDataset(_sample_x_train_padded, _sample_y_train_np)\n_val_data = TextDataset(_sample_x_val_padded, _sample_y_val_np)\n\n_train_data[0], len(_train_data)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:38:49.941513Z","iopub.execute_input":"2024-04-14T14:38:49.941903Z","iopub.status.idle":"2024-04-14T14:38:49.961752Z","shell.execute_reply.started":"2024-04-14T14:38:49.941868Z","shell.execute_reply":"2024-04-14T14:38:49.960794Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"((tensor([   743,      3,    147,    379,     43,    582,    988,   2965,      2,\n             931,   2094,   1096,     17,     13,     97,   7374,      2,     86,\n             114,      5,    112,      5,     21,    327,      1,    466,     12,\n               4,  15040,    202,   6500,     10,     13,    109,    704,    163,\n            7449,     13,    296,      7,      1,     31,    202,   6500,   3421,\n              13,     97,   7374,      6,     26,    559,      4,     80,     22,\n               1,   1380,      5,    347,    251,      4,   1018,   2301, 591609,\n               6,    159,      2,    557,     23,    410,   1214,    471,     75,\n             156,     15,     24,   2965,     36,      5,    145,    117,     15,\n              24,    533,      2,     37,     15,    412,    247,     81,    889,\n              62,   5662,     10,      3,   2094,    704,     12,    142,    519,\n               2,     24,   4859,    598,      2,    325, 663419,    159,   1502,\n             410,     78,   1808,     17, 524549,   2010,    308,     86,     21,\n              58,   3973,     14,     86,    102,    162,     13,   2093,     27,\n             538,    375,    514,   2093,      2,      8,    160,     34,      3,\n              30,   1387,    137,      5,   2673,    139,      3,   2010,     17,\n             309,    218, 491348,     97,   3710,      9,   2965,      4,    236,\n            1449,      9,    120,      2,      3,    320,   1096,   1278,     71,\n             394,      5,    493,    901,      0,      0,      0,      0,      0,\n               0,      0,      0,      0,      0,      0,      0,      0,      0,\n               0,      0,      0,      0,      0,      0,      0,      0,      0,\n               0,      0,      0,      0,      0,      0,      0,      0,      0,\n               0,      0]),\n  3),\n 150)"},"metadata":{}}]},{"cell_type":"code","source":"# Of course make dataloaders, batch size a bit arbitrary as well\n# _batch_size = 250\n# _batch_size = 1100\n_batch_size = 50\n\n_train_loader = DataLoader(_train_data + _val_data, batch_size=_batch_size, shuffle=True)\n_val_loader = DataLoader(_train_data + _val_data, batch_size=_batch_size, shuffle=True)\n\nlen(_train_loader)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:38:49.962768Z","iopub.execute_input":"2024-04-14T14:38:49.962998Z","iopub.status.idle":"2024-04-14T14:38:49.984229Z","shell.execute_reply.started":"2024-04-14T14:38:49.962979Z","shell.execute_reply":"2024-04-14T14:38:49.983380Z"},"trusted":true},"execution_count":62,"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"6"},"metadata":{}}]},{"cell_type":"code","source":"# Necessary variables for model\nvocab_size = len(vocab.get_stoi()) + 1\nembedding_dim = 200\nnum_classes = 5","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:38:49.985195Z","iopub.execute_input":"2024-04-14T14:38:49.985442Z","iopub.status.idle":"2024-04-14T14:38:51.556397Z","shell.execute_reply.started":"2024-04-14T14:38:49.985422Z","shell.execute_reply":"2024-04-14T14:38:51.555525Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"''' The previous implementation had a specific LstmModelOptuna which was made to experiment with num_layers arg\n    of the nn.LSTM layer. The primary focus is now on the regularization alpha/lambda instead. So no new model\n    class, just the define function. '''\n    \ndef define_model(trial):\n    optuna_lstm_model = LSTMModel(vocab_size=vocab_size,\n                                  embedding_dim=embedding_dim,\n                                  max_padding_len=max_padding_len,\n                                  num_classes=num_classes).to(device)\n    \n    return optuna_lstm_model","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:38:51.579326Z","iopub.execute_input":"2024-04-14T14:38:51.579620Z","iopub.status.idle":"2024-04-14T14:38:51.595435Z","shell.execute_reply.started":"2024-04-14T14:38:51.579598Z","shell.execute_reply":"2024-04-14T14:38:51.594526Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"''' Normally Optuna is used with a regular Object FUNCTION. See https://optuna.readthedocs.io/en/stable/reference/generated/optuna.trial.Trial.html \n    for the first example of a \"objective(trial)\"\n    \n    But it's possible to use classes and I find them useful because I can save variables as I do later in the project.\n    The below code takes in a simple min and max for one class object (that'll be SAVED by Optuna and used with every trial) and\n    called repeatedly.\n    \n    Below is just an example of that. In steps what happens is:\n        1) study object created so we can run a test func repeatedly.\n        2) The class object is created and saved within the study object itself for repeated use. \n        3) The class object here is given 2 variables to save, min and max\n        4) n_trials means we'll run this func 3 times.\n        5) __call___ will be of course, called however many times n_trial states.\n        6) The print statement shows the saved values.\n        7) Return a value with the simple math equation x * 2.\n    \n    '''\n\nclass ObjectiveClassTest:\n    def __init__(self, min_x, max_x):\n        # Hold this implementation specific arguments as the fields of the class.\n        self.min_x = min_x\n        self.max_x = max_x\n\n    def __call__(self, trial):\n        print(f'__call__ func is called! min: {self.min_x}. max: {self.max_x}')\n        # Calculate an objective value by using the extra arguments.\n        x = trial.suggest_float(\"x\", self.min_x, self.max_x)\n        return x * 2\n\n\n# Execute an optimization by using an `Objective` instance.\nstudy = optuna.create_study()\nstudy.optimize(ObjectiveClassTest(1, 5), n_trials=3)","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:38:51.596567Z","iopub.execute_input":"2024-04-14T14:38:51.596879Z","iopub.status.idle":"2024-04-14T14:38:51.614626Z","shell.execute_reply.started":"2024-04-14T14:38:51.596837Z","shell.execute_reply":"2024-04-14T14:38:51.613770Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stderr","text":"[I 2024-04-14 14:38:51,604] A new study created in memory with name: no-name-e110697f-d651-48c6-acd4-7cbca6ba36b3\n[I 2024-04-14 14:38:51,607] Trial 0 finished with value: 2.2285601497045313 and parameters: {'x': 1.1142800748522657}. Best is trial 0 with value: 2.2285601497045313.\n[I 2024-04-14 14:38:51,608] Trial 1 finished with value: 5.937333894763212 and parameters: {'x': 2.968666947381606}. Best is trial 0 with value: 2.2285601497045313.\n[I 2024-04-14 14:38:51,610] Trial 2 finished with value: 3.923274561831561 and parameters: {'x': 1.9616372809157805}. Best is trial 0 with value: 2.2285601497045313.\n","output_type":"stream"},{"name":"stdout","text":"__call__ func is called! min: 1. max: 5\n__call__ func is called! min: 1. max: 5\n__call__ func is called! min: 1. max: 5\n","output_type":"stream"}]},{"cell_type":"code","source":"import gc\n\nclass Objective(object):\n    def __init__(self, min_lambda, max_lambda, learning_rate):\n        # Get the min and max range for the l1 alpha/lambda\n        self.min_val = min_lambda\n        self.max_val = max_lambda\n        \n        # Checking if better models have increased validation accuracy for debugging purposes\n        self.val_accuracy = 0\n        \n        # Init loss func in here. Same one will be used. \n        self.criterion = nn.CrossEntropyLoss() \n        \n        # Optimizer will be initialized in call func but the learning rate is saved here for it.\n        self.learning_rate = learning_rate\n        \n    \n    # If a class object is created, then ex: \"classobj()\" is done, below function is called.\n    def __call__(self, trial):\n        # Make model. PLACE ON DEVICE\n        l1_model = define_model(trial).to(device)\n        \n        # Get a value between min and max\n        new_l1_lambda = trial.suggest_float(\"new_l1_lambda\", self.min_val, self.max_val)\n        \n        # Init optimizer here since every one needs the current models parameters.\n        cur_optimizer = torch.optim.Adam(l1_model.parameters(), lr=self.learning_rate) \n        \n        ''' Calculate the weights again because remember last 3 arguments of the train function are: use_l1=False, n_weights=0, l1_lambda=0. \n            1) use_l1 - Must be true, because using Optuna to tune the l1 lambda is the entire point.\n            2) n_weights - Calculating below.\n            3) l1_lambda - This is what the variable \"new_l1_lambda\" is for. '''\n        new_l1_weights = 0\n        for name, weights in l1_model.named_parameters():\n            if 'bias' not in name:\n                new_l1_weights = new_l1_weights + weights.numel()\n        \n        first_loop_epochs = 1\n        for i in range(first_loop_epochs):        \n            l_train_loss, l_train_acc, l_val_loss, l_val_acc = train(model=l1_model, \n                                                                     epochs=50, \n                                                                     train_loader=_train_loader, \n                                                                     optimizer=cur_optimizer, \n                                                                     criterion=self.criterion,\n                                                                     val_loader=_val_loader,\n                                                                     device=device,\n                                                                     display=False,\n                                                                     use_l1=True,\n                                                                     n_weights=new_l1_weights,\n                                                                     l1_lambda=new_l1_lambda)\n            \n            cur_val_accuracy = np.sum(l_val_acc) / len(l_val_acc)\n\n            if cur_val_accuracy > self.val_accuracy:\n                print(f'{cur_val_accuracy} > {self.val_accuracy}, updating best val accuracy now,\\n')\n                self.val_accuracy = cur_val_accuracy\n                \n                \n            # Update Optuna on how well this first_loop_epoch went\n            trial.report(cur_val_accuracy, i)\n        \n            # Pruning basically stops continuing with this model training if its not improving.\n            if trial.should_prune():\n                raise optuna.exceptions.TrialPruned()\n                \n        l1_model.cpu()\n        del l1_model\n        gc.collect()\n        torch.cuda.empty_cache()\n                \n        return self.val_accuracy","metadata":{"execution":{"iopub.status.busy":"2024-04-14T14:38:51.615876Z","iopub.execute_input":"2024-04-14T14:38:51.616283Z","iopub.status.idle":"2024-04-14T14:38:51.630781Z","shell.execute_reply.started":"2024-04-14T14:38:51.616260Z","shell.execute_reply":"2024-04-14T14:38:51.629903Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"study = optuna.create_study(direction='maximize')\n\n_trials = 10\n\n\n'''\n                        !!! IMPORTANT !!!\n    The min and max are critcal. Why? Earlier I said: \"Both l1/l2 needs alpha argument. Determines how much \n    attention to pay to the penalty of regularization. Penalty strong? You'll regularize too hard. Weights \n    will be small, underfitting happens. Penalty soft? You won't regularize much, weights will go unpunished \n    and overfitting still occurs.\" \n    \n    With that said, how much to regularize is tricky. The expected values are 0.1 to 0.001 but don't bank\n    on that. Remember the smaller the punishment, the more regular overfitting occurs. A process of \n    experimentation for the best value is necessary. Hence why Optuna is used!\n    \n    I googled: \"l1 regularization lambda value\"\n    \n    1) https://www.linkedin.com/pulse/bxd-primer-series-lasso-regression-models-l1-general-comparison-k-/\n        \"However, these are just rough guidelines, and the optimal value of  for a particular problem \n         should be determined through experimentation and cross-validation. It is often useful to try a \n         range of hyper-parameter values and select the one that gives the best performance on a validation set.\"\n    \n    2) https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/lambda\n        \"The ideal value of lambda produces a model that generalizes well to new, previously unseen data. \n         Unfortunately, that ideal value of lambda is data-dependent, so you'll need to do some tuning. \"\n    \n    Must make sure to give the desired min and max for the l1 lambda to be tuned.\n    \n    '''\ncur_min = 2.5\ncur_max = 5\nstudy.optimize(Objective(cur_min, cur_max, learning_rate), n_trials=_trials, timeout=600)\n\n\npruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\ncompleted_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n\nprint(\"Study statistics:\\n\")\nprint(f'Number of finished trials: {len(study.trials)}')\nprint(f'Number of pruned trials: {len(pruned_trials)}')\nprint(f'Number of complete trials: {len(completed_trials)}\\n\\n')\n\nbest_trial = study.best_trial\nprint(f'Best trial: {best_trial}\\n\\n')\n\nprint('Params:')\nfor key, value in best_trial.params.items():\n    print('{} - {}'.format(key, value))","metadata":{"execution":{"iopub.status.busy":"2024-04-14T15:03:46.862002Z","iopub.execute_input":"2024-04-14T15:03:46.862944Z","iopub.status.idle":"2024-04-14T15:09:24.683273Z","shell.execute_reply.started":"2024-04-14T15:03:46.862908Z","shell.execute_reply":"2024-04-14T15:09:24.682256Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stderr","text":"[I 2024-04-14 15:03:46,870] A new study created in memory with name: no-name-87a880fb-339b-416f-9f37-1e25ebddc901\n","output_type":"stream"},{"name":"stdout","text":"45.318712891064955 > 0, updating best val accuracy now,\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-04-14 15:04:21,942] Trial 0 finished with value: 45.318712891064955 and parameters: {'new_l1_lambda': 3.283220887137127}. Best is trial 0 with value: 45.318712891064955.\n[I 2024-04-14 15:04:57,012] Trial 1 finished with value: 45.318712891064955 and parameters: {'new_l1_lambda': 3.4899168879812876}. Best is trial 0 with value: 45.318712891064955.\n[I 2024-04-14 15:05:32,019] Trial 2 finished with value: 45.318712891064955 and parameters: {'new_l1_lambda': 4.570049495408927}. Best is trial 0 with value: 45.318712891064955.\n[I 2024-04-14 15:06:07,025] Trial 3 finished with value: 45.318712891064955 and parameters: {'new_l1_lambda': 4.098175030593161}. Best is trial 0 with value: 45.318712891064955.\n[I 2024-04-14 15:06:42,066] Trial 4 finished with value: 45.318712891064955 and parameters: {'new_l1_lambda': 2.8708297448176725}. Best is trial 0 with value: 45.318712891064955.\n[I 2024-04-14 15:07:17,120] Trial 5 finished with value: 45.318712891064955 and parameters: {'new_l1_lambda': 3.2792568059525484}. Best is trial 0 with value: 45.318712891064955.\n","output_type":"stream"},{"name":"stdout","text":"45.39143154071756 > 45.318712891064955, updating best val accuracy now,\n\n","output_type":"stream"},{"name":"stderr","text":"[I 2024-04-14 15:07:52,155] Trial 6 finished with value: 45.39143154071756 and parameters: {'new_l1_lambda': 3.5962595752999396}. Best is trial 6 with value: 45.39143154071756.\n[I 2024-04-14 15:08:23,011] Trial 7 pruned. \n[I 2024-04-14 15:08:53,867] Trial 8 pruned. \n[I 2024-04-14 15:09:24,675] Trial 9 pruned. \n","output_type":"stream"},{"name":"stdout","text":"Study statistics:\n\nNumber of finished trials: 10\nNumber of pruned trials: 3\nNumber of complete trials: 7\n\n\nBest trial: FrozenTrial(number=6, state=TrialState.COMPLETE, values=[45.39143154071756], datetime_start=datetime.datetime(2024, 4, 14, 15, 7, 17, 121649), datetime_complete=datetime.datetime(2024, 4, 14, 15, 7, 52, 155507), params={'new_l1_lambda': 3.5962595752999396}, user_attrs={}, system_attrs={}, intermediate_values={0: 45.39143154071756}, distributions={'new_l1_lambda': FloatDistribution(high=5.0, log=False, low=2.5, step=None)}, trial_id=6, value=None)\n\n\nParams:\nnew_l1_lambda - 3.5962595752999396\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 13) Conclusion Notes & Realization","metadata":{}},{"cell_type":"code","source":"''' 1) Dataset -\n        The primary concern about the dataset was the issue with overfitting and that has been addressed with L1. To remind why L1 was chosen, a quote from \n        my previous comments: \"L1 will be implemented. Why? L2 will make things worse if the weights are high. If variance is the issue, then the weights \n        are ALREADY high. Last thing needed is the squared weights to make them higher.\"\n    \n    2) Lambda - \n        The lambda value effects the implact of L1. If anyone reading this decides to have a go and mess around with the code, feel free to use different \n        values. Maybe bigger, maybe smaller. I admit I have a preconceived notion that regularization would impact results at the same rate no matter what.\n        I'm very glad taking on this project has given me the realization that the notion was HORRIFICALLY wrong.\n        \n    3) Optuna -\n        Optuna obviously tunes hyperparameters of a model and I just used it for a new lambda which is fine. To be honest, I completed an entirely DIFFERENT\n        project to use Optuna JUST to see its results so I can apply what Optuna can do in THIS project. Sounds crazy but here is the link to prove\n        it: https://github.com/axiom2018/AG-News-Classification '''","metadata":{"execution":{"iopub.status.busy":"2024-04-14T15:44:00.476071Z","iopub.execute_input":"2024-04-14T15:44:00.477000Z","iopub.status.idle":"2024-04-14T15:44:00.483063Z","shell.execute_reply.started":"2024-04-14T15:44:00.476967Z","shell.execute_reply":"2024-04-14T15:44:00.482015Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stdout","text":"\n","output_type":"stream"}]}]}